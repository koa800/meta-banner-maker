#!/usr/bin/env python3
"""
CSV â†’ Google Sheets è‡ªå‹•åŒæœŸã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ã€ŒLooker Studio CSVã€ãƒ•ã‚©ãƒ«ãƒ€å†…ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡ºã—ã€
ã€Œå…ƒãƒ‡ãƒ¼ã‚¿ã€ã‚·ãƒ¼ãƒˆã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’è‡ªå‹•æ›´æ–°ã™ã‚‹ã€‚

å‹•ä½œ:
  1. ãƒ•ã‚©ãƒ«ãƒ€å†…ã®æ—¥ä»˜ä»˜ãCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³
  2. ã‚·ãƒ¼ãƒˆã®ã€Œè¦ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã€è¡Œã¨ãƒãƒƒãƒãƒ³ã‚°
  3. ãƒãƒƒãƒã—ãŸè¡Œã®æŠ•å…¥æ—¥æ™‚ãƒ»ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ã€Œå®Œäº†ã€ã«æ›´æ–°
  4. æ—¥ä»˜ä¸æ˜ã®CSVï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåï¼‰ã‚’æ¤œå‡ºã—ãŸã‚‰LINEã§é€šçŸ¥

ä½¿ã„æ–¹:
  python3 csv_sheet_sync.py              # å…ƒãƒ‡ãƒ¼ã‚¿ã‚·ãƒ¼ãƒˆåŒæœŸ
  python3 csv_sheet_sync.py --dry-run    # ç¢ºèªã®ã¿ï¼ˆæ›¸ãè¾¼ã¿ã—ãªã„ï¼‰
  python3 csv_sheet_sync.py build        # ã‚¹ã‚­ãƒ«ãƒ—ãƒ©ã‚¹ï¼ˆæ—¥åˆ¥ï¼‰ã‚·ãƒ¼ãƒˆã‚’å…¨CSV ã‹ã‚‰æ§‹ç¯‰
  python3 csv_sheet_sync.py build --dry-run
  python3 csv_sheet_sync.py monthly      # ã‚¹ã‚­ãƒ«ãƒ—ãƒ©ã‚¹ï¼ˆæœˆåˆ¥ï¼‰ã‚·ãƒ¼ãƒˆã‚’æ—¥åˆ¥ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é›†è¨ˆ
  python3 csv_sheet_sync.py monthly --dry-run
  python3 csv_sheet_sync.py cache        # KPIã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã¿å†ç”Ÿæˆ
"""

import os
import sys
import re
import csv
import json
import logging
import tempfile
import requests
from datetime import datetime, timedelta

# sheets_manager ã¨åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, BASE_DIR)
from sheets_manager import get_client, extract_spreadsheet_id

# â”€â”€â”€ è¨­å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CSV_DIR = os.path.expanduser("~/Desktop/Looker Studio CSV")
SPREADSHEET_ID = "1FOh_XGZWaEisfFEngiN848kSm2E6HotAZiMDTmO7BNA"
SPREADSHEET_URL = f"https://docs.google.com/spreadsheets/d/{SPREADSHEET_ID}/edit?gid=1948910703"
SHEET_NAME = "å…ƒãƒ‡ãƒ¼ã‚¿"
DAILY_SHEET_NAME = "ã‚¹ã‚­ãƒ«ãƒ—ãƒ©ã‚¹ï¼ˆæ—¥åˆ¥ï¼‰"
MONTHLY_SHEET_NAME = "ã‚¹ã‚­ãƒ«ãƒ—ãƒ©ã‚¹ï¼ˆæœˆåˆ¥ï¼‰"

# KPIã‚µãƒãƒªãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥
KPI_CACHE_PATH = os.path.join(BASE_DIR, "data", "kpi_summary.json")
ACCOUNT = "kohara"
BASE_CSV_NAME = "ã‚¢ãƒ‰ãƒã‚¹å…¨ä½“æ•°å€¤_åª’ä½“ãƒ»ãƒ•ã‚¡ãƒãƒ«åˆ¥ãƒ‡ãƒ¼ã‚¿_è¡¨"

# LINEé€šçŸ¥è¨­å®š
CONFIG_PATH = os.path.join(BASE_DIR, "line_bot_local", "config.json")
SERVER_URL = "https://line-mention-bot-mmzu.onrender.com"
AGENT_TOKEN = ""
if os.path.exists(CONFIG_PATH):
    with open(CONFIG_PATH) as f:
        _cfg = json.load(f)
        AGENT_TOKEN = _cfg.get("agent_token", "")

# é€šçŸ¥æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®è¨˜éŒ²ï¼ˆåŒã˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½•åº¦ã‚‚é€šçŸ¥ã—ãªã„ï¼‰
NOTIFIED_FILE = os.path.join(BASE_DIR, "csv_sheet_sync_notified.json")

# ãƒ­ã‚°è¨­å®š
LOG_FILE = os.path.join(BASE_DIR, "csv_sheet_sync.log")
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(LOG_FILE, encoding="utf-8"),
        logging.StreamHandler(),
    ],
)
logger = logging.getLogger(__name__)

# æ—¥ä»˜ä»˜ããƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³: 2025-07-01_ã‚¢ãƒ‰ãƒã‚¹å…¨ä½“æ•°å€¤_...csv
DATE_PATTERN = re.compile(r'^(\d{4}-\d{2}-\d{2})_(.+)\.csv$')


# â”€â”€â”€ LINEé€šçŸ¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def send_line_notify(message: str) -> bool:
    """LINEç§˜æ›¸ã‚°ãƒ«ãƒ¼ãƒ—ã«é€šçŸ¥ã‚’é€ã‚‹"""
    if not AGENT_TOKEN:
        logger.warning("AGENT_TOKENæœªè¨­å®š: LINEé€šçŸ¥ã‚’ã‚¹ã‚­ãƒƒãƒ—")
        return False
    try:
        resp = requests.post(
            f"{SERVER_URL}/notify",
            headers={"Authorization": f"Bearer {AGENT_TOKEN}"},
            json={"message": message},
            timeout=40,
        )
        if resp.status_code == 200:
            logger.info("LINEé€šçŸ¥é€ä¿¡å®Œäº†")
            return True
        else:
            logger.error(f"LINEé€šçŸ¥å¤±æ•—: {resp.status_code} {resp.text[:200]}")
            return False
    except Exception as e:
        logger.error(f"LINEé€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def load_notified():
    """é€šçŸ¥æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚€"""
    if os.path.exists(NOTIFIED_FILE):
        with open(NOTIFIED_FILE) as f:
            return set(json.load(f))
    return set()


def save_notified(notified: set):
    """é€šçŸ¥æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’ä¿å­˜ã™ã‚‹"""
    with open(NOTIFIED_FILE, "w") as f:
        json.dump(sorted(notified), f, ensure_ascii=False)


# â”€â”€â”€ ã‚¹ã‚­ãƒ£ãƒ³ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def scan_csv_folder():
    """ãƒ•ã‚©ãƒ«ãƒ€å†…ã®æ—¥ä»˜ä»˜ãCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã¦ {æ—¥ä»˜: ãƒ•ã‚¡ã‚¤ãƒ«å} ã‚’è¿”ã™"""
    if not os.path.isdir(CSV_DIR):
        logger.error(f"ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {CSV_DIR}")
        return {}, []

    dated_files = {}
    unnamed_files = []

    for f in os.listdir(CSV_DIR):
        if not f.endswith(".csv") or BASE_CSV_NAME not in f:
            continue

        m = DATE_PATTERN.match(f)
        if m:
            dated_files[m.group(1)] = f
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåï¼ˆæ—¥ä»˜ãªã—ï¼‰ã®CSV
            unnamed_files.append(f)

    return dated_files, unnamed_files


def auto_rename_unnamed_files(unnamed_files: list, dated_files: dict, dry_run: bool = False) -> list:
    """æ—¥ä»˜ä¸æ˜ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•ãƒªãƒãƒ¼ãƒ ã™ã‚‹ã€‚

    æ—¢å­˜ã®æ—¥ä»˜ä»˜ãCSVã‹ã‚‰æœ€æ–°æ—¥ä»˜ã‚’å–å¾—ã—ã€ç¿Œæ—¥ä»¥é™ã‚’å‰²ã‚Šå½“ã¦ã‚‹ã€‚
    Returns: ãƒªãƒãƒ¼ãƒ ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒªã‚¹ãƒˆ
    """
    if not unnamed_files:
        return []

    # æ—¢å­˜ã®æ—¥ä»˜ä»˜ãCSVã‹ã‚‰æœ€æ–°æ—¥ä»˜ã‚’ç‰¹å®š
    if dated_files:
        latest_date = max(datetime.strptime(d, "%Y-%m-%d") for d in dated_files)
    else:
        # æ—¥ä»˜ä»˜ãCSVãŒãªã„å ´åˆã¯æ˜¨æ—¥ã‚’åŸºæº–ã«ã™ã‚‹
        latest_date = datetime.now() - timedelta(days=1)

    # ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°æ—¥æ™‚é †ã«ã‚½ãƒ¼ãƒˆï¼ˆå¤ã„é †ï¼‰
    unnamed_with_mtime = []
    for f in unnamed_files:
        path = os.path.join(CSV_DIR, f)
        mtime = os.path.getmtime(path)
        unnamed_with_mtime.append((f, mtime))
    unnamed_with_mtime.sort(key=lambda x: x[1])

    renamed = []
    next_date = latest_date + timedelta(days=1)

    for fname, _ in unnamed_with_mtime:
        date_str = next_date.strftime("%Y-%m-%d")
        new_name = f"{date_str}_{fname}"
        old_path = os.path.join(CSV_DIR, fname)
        new_path = os.path.join(CSV_DIR, new_name)

        if dry_run:
            logger.info(f"(dry-run) ãƒªãƒãƒ¼ãƒ äºˆå®š: {fname} â†’ {new_name}")
        else:
            os.rename(old_path, new_path)
            logger.info(f"è‡ªå‹•ãƒªãƒãƒ¼ãƒ : {fname} â†’ {new_name}")
            renamed.append(new_name)

        next_date += timedelta(days=1)

    # LINEé€šçŸ¥
    if renamed and not dry_run:
        dates = [DATE_PATTERN.match(f).group(1) for f in renamed]
        if len(dates) == 1:
            date_info = dates[0]
        else:
            date_info = f"{dates[0]} ã€œ {dates[-1]}"
        message = (
            f"ğŸ“Š CSVè‡ªå‹•ãƒªãƒãƒ¼ãƒ : {len(renamed)} ä»¶\n"
            f"{date_info} ã¨ã—ã¦å‡¦ç†ã—ã¾ã™"
        )
        send_line_notify(message)

    return renamed


# â”€â”€â”€ ã‚·ãƒ¼ãƒˆåŒæœŸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def sync_to_sheet(dry_run=False):
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã‚·ãƒ¼ãƒˆã‚’åŒæœŸ"""
    # 1. ãƒ•ã‚©ãƒ«ãƒ€ã‚¹ã‚­ãƒ£ãƒ³
    csv_files, unnamed_files = scan_csv_folder()

    # æ—¥ä»˜ä¸æ˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•ãƒªãƒãƒ¼ãƒ 
    renamed = auto_rename_unnamed_files(unnamed_files, csv_files, dry_run=dry_run)

    # ãƒªãƒãƒ¼ãƒ ãŒã‚ã£ãŸå ´åˆã¯å†ã‚¹ã‚­ãƒ£ãƒ³
    if renamed:
        csv_files, unnamed_files = scan_csv_folder()

    if not csv_files:
        logger.info("æ—¥ä»˜ä»˜ãCSVãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
        return 0

    logger.info(f"æ—¥ä»˜ä»˜ãCSV: {len(csv_files)} ãƒ•ã‚¡ã‚¤ãƒ«")

    # 2. ã‚·ãƒ¼ãƒˆèª­ã¿è¾¼ã¿
    spreadsheet_id, gid = extract_spreadsheet_id(SPREADSHEET_URL)
    client = get_client(ACCOUNT)
    spreadsheet = client.open_by_key(spreadsheet_id)
    ws = next((w for w in spreadsheet.worksheets() if w.id == gid), None)
    if ws is None:
        ws = spreadsheet.worksheet(SHEET_NAME)

    data = ws.get_all_values()
    if not data:
        logger.error("ã‚·ãƒ¼ãƒˆãŒç©ºã§ã™")
        return 0

    # 2.5. ã‚·ãƒ¼ãƒˆã«ãªã„æ—¥ä»˜ã®CSVãŒã‚ã‚Œã°è¡Œã‚’è‡ªå‹•è¿½åŠ 
    existing_dates = {row[0] for row in data[1:]}  # ãƒ˜ãƒƒãƒ€ãƒ¼é™¤ã
    new_dates = sorted(d for d in csv_files if d not in existing_dates)

    if new_dates and not dry_run:
        new_rows = [
            [d, f"{BASE_CSV_NAME}.csv", "", "è¦ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"]
            for d in new_dates
        ]
        ws.append_rows(new_rows, value_input_option="USER_ENTERED")
        logger.info(f"å…ƒãƒ‡ãƒ¼ã‚¿ã«æ–°è¦è¡Œè¿½åŠ : {len(new_dates)} ä»¶ ({new_dates[0]} ã€œ {new_dates[-1]})")
        # è¿½åŠ å¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚’å†èª­ã¿è¾¼ã¿
        data = ws.get_all_values()
    elif new_dates:
        logger.info(f"(dry-run) å…ƒãƒ‡ãƒ¼ã‚¿ã«æ–°è¦è¡Œè¿½åŠ äºˆå®š: {len(new_dates)} ä»¶ ({new_dates[0]} ã€œ {new_dates[-1]})")

    # 3. æ›´æ–°å¯¾è±¡ã‚’ç‰¹å®š
    now = datetime.now().strftime("%Y-%m-%d %H:%M")
    updates = []
    update_rows = []

    for i, row in enumerate(data[1:], start=2):  # skip header
        target_date = row[0]
        status = row[3] if len(row) > 3 else ""

        if status == "å®Œäº†":
            continue  # æ—¢ã«å®Œäº†æ¸ˆã¿ã¯ã‚¹ã‚­ãƒƒãƒ—

        if target_date in csv_files:
            filename = csv_files[target_date]
            updates.append({
                "row": i,
                "date": target_date,
                "filename": filename,
            })
            update_rows.append([filename, now, "å®Œäº†"])

    if not updates:
        logger.info("æ›´æ–°å¯¾è±¡ãªã—ï¼ˆã™ã¹ã¦å®Œäº†æ¸ˆã¿ or CSVãªã—ï¼‰")
        return 0

    logger.info(f"æ›´æ–°å¯¾è±¡: {len(updates)} è¡Œ")
    for u in updates[:5]:
        logger.info(f"  {u['date']} â†’ {u['filename']}")
    if len(updates) > 5:
        logger.info(f"  ... ä»– {len(updates) - 5} è¡Œ")

    if dry_run:
        logger.info("(dry-run: æ›¸ãè¾¼ã¿ã‚¹ã‚­ãƒƒãƒ—)")
        return len(updates)

    # 4. ä¸€æ‹¬æ›¸ãè¾¼ã¿
    first_row = updates[0]["row"]
    last_row = updates[-1]["row"]

    if last_row - first_row + 1 == len(updates):
        # é€£ç¶šè¡Œ â†’ ä¸€æ‹¬æ›´æ–°
        range_notation = f"B{first_row}:D{last_row}"
        ws.update(values=update_rows, range_name=range_notation)
        logger.info(f"ä¸€æ‹¬æ›¸ãè¾¼ã¿å®Œäº†: {range_notation}")
    else:
        # é£›ã³é£›ã³ â†’ å…¨è¡Œåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½œã£ã¦ä¸€æ‹¬æ›´æ–°
        all_rows = []
        update_map = {u["row"]: idx for idx, u in enumerate(updates)}
        for i, row in enumerate(data[1:], start=2):
            if i in update_map:
                all_rows.append(update_rows[update_map[i]])
            else:
                all_rows.append([row[1] if len(row) > 1 else "",
                                 row[2] if len(row) > 2 else "",
                                 row[3] if len(row) > 3 else ""])
        range_notation = f"B2:D{len(data)}"
        ws.update(values=all_rows, range_name=range_notation)
        logger.info(f"ä¸€æ‹¬æ›¸ãè¾¼ã¿å®Œäº†: {range_notation}")

    # 5. æ›´æ–°å®Œäº†ã‚’LINEé€šçŸ¥
    dates = [u["date"] for u in updates]
    first = dates[0]
    last = dates[-1]
    message = f"ğŸ“Š å…ƒãƒ‡ãƒ¼ã‚¿ã‚·ãƒ¼ãƒˆæ›´æ–°å®Œäº†: {len(updates)} ä»¶\n{first} ã€œ {last}"
    send_line_notify(message)

    return len(updates)


# â”€â”€â”€ ã‚¹ã‚­ãƒ«ãƒ—ãƒ©ã‚¹ï¼ˆæ—¥åˆ¥ï¼‰æ§‹ç¯‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def parse_number(val):
    """CSV ã®å€¤ã‚’æ•°å€¤ã«å¤‰æ›ã€‚ç©ºæ–‡å­—ãƒ»å¤‰æ›ä¸å¯ã¯ 0"""
    if not val or val.strip() == "":
        return 0
    try:
        n = float(val)
        return int(n) if n == int(n) else round(n, 2)
    except (ValueError, OverflowError):
        return 0


def read_all_csvs():
    """å…¨CSVã‚’èª­ã¿è¾¼ã¿ã€æ—¥ä»˜ä»˜ãã®è¡Œãƒªã‚¹ãƒˆã‚’è¿”ã™"""
    if not os.path.isdir(CSV_DIR):
        logger.error(f"ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {CSV_DIR}")
        return []

    all_rows = []
    files = sorted(f for f in os.listdir(CSV_DIR)
                   if DATE_PATTERN.match(f) and BASE_CSV_NAME in f)

    for fname in files:
        date_str = DATE_PATTERN.match(fname).group(1)
        path = os.path.join(CSV_DIR, fname)
        with open(path, encoding="utf-8-sig") as fh:
            reader = csv.reader(fh)
            header = next(reader, None)
            if not header:
                continue
            for row in reader:
                if len(row) < 12:
                    continue
                # [æ—¥ä»˜, å¤§ã‚«ãƒ†ã‚´ãƒª, é›†å®¢åª’ä½“, ãƒ•ã‚¡ãƒãƒ«å,
                #  é›†å®¢æ•°, å€‹åˆ¥äºˆç´„æ•°, å®Ÿæ–½æ•°, å£²ä¸Š, åºƒå‘Šè²»,
                #  CPA, å€‹åˆ¥CPO, å˜æœˆROAS, å˜æœˆLTV]
                all_rows.append([
                    date_str,
                    row[0],                # å¤§ã‚«ãƒ†ã‚´ãƒª
                    row[1],                # é›†å®¢åª’ä½“
                    row[2],                # ãƒ•ã‚¡ãƒãƒ«å
                    parse_number(row[3]),   # é›†å®¢æ•°
                    parse_number(row[4]),   # å€‹åˆ¥äºˆç´„æ•°
                    parse_number(row[5]),   # å®Ÿæ–½æ•°
                    parse_number(row[6]),   # å£²ä¸Š
                    parse_number(row[7]),   # åºƒå‘Šè²»
                    parse_number(row[8]),   # CPA
                    parse_number(row[9]),   # å€‹åˆ¥CPO
                    parse_number(row[10]),  # å˜æœˆROAS
                    parse_number(row[11]),  # å˜æœˆLTV
                ])

    logger.info(f"CSVèª­ã¿è¾¼ã¿: {len(files)} ãƒ•ã‚¡ã‚¤ãƒ«, {len(all_rows)} è¡Œ")
    return all_rows


def build_daily_sheet(dry_run=False):
    """å…¨CSVãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€ã‚¹ã‚­ãƒ«ãƒ—ãƒ©ã‚¹ï¼ˆæ—¥åˆ¥ï¼‰ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã‚€"""
    # 1. å…¨CSVèª­ã¿è¾¼ã¿
    all_rows = read_all_csvs()
    if not all_rows:
        logger.error("CSVãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return 0

    # æ—¥ä»˜ã®ç¯„å›²
    dates = sorted(set(r[0] for r in all_rows))
    logger.info(f"æœŸé–“: {dates[0]} ã€œ {dates[-1]} ({len(dates)} æ—¥)")
    logger.info(f"åˆè¨ˆ: {len(all_rows)} è¡Œ")

    if dry_run:
        logger.info("(dry-run: æ›¸ãè¾¼ã¿ã‚¹ã‚­ãƒƒãƒ—)")
        return len(all_rows)

    # 2. ã‚·ãƒ¼ãƒˆã«æ¥ç¶š
    client = get_client(ACCOUNT)
    spreadsheet = client.open_by_key(SPREADSHEET_ID)
    ws = spreadsheet.worksheet(DAILY_SHEET_NAME)

    # 3. ã‚·ãƒ¼ãƒˆã‚’ãƒªã‚µã‚¤ã‚ºï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼3è¡Œ + ãƒ‡ãƒ¼ã‚¿è¡Œ + ä½™è£•100è¡Œï¼‰
    # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ: è¡Œ1=æœ€çµ‚æ›´æ–°, è¡Œ2=ç©º, è¡Œ3=ãƒ˜ãƒƒãƒ€ãƒ¼, è¡Œ4ã€œ=ãƒ‡ãƒ¼ã‚¿
    needed_rows = 3 + len(all_rows) + 100
    current_rows = ws.row_count
    if needed_rows > current_rows:
        ws.resize(rows=needed_rows)
        logger.info(f"ã‚·ãƒ¼ãƒˆãƒªã‚µã‚¤ã‚º: {current_rows} â†’ {needed_rows} è¡Œ")

    # 4. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢ï¼ˆè¡Œ1ä»¥é™ã™ã¹ã¦ï¼‰
    ws.batch_clear([f"A1:M{current_rows}"])
    logger.info("æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªã‚¢å®Œäº†")

    # 5. ãƒ‡ãƒ¼ã‚¿æ›¸ãè¾¼ã¿ï¼ˆè¡Œ4ã€œï¼‰
    BATCH_SIZE = 1000
    for i in range(0, len(all_rows), BATCH_SIZE):
        batch = all_rows[i:i + BATCH_SIZE]
        start_row = 4 + i
        end_row = start_row + len(batch) - 1
        range_notation = f"A{start_row}:M{end_row}"
        ws.update(values=batch, range_name=range_notation, value_input_option="USER_ENTERED")
        logger.info(f"æ›¸ãè¾¼ã¿: {range_notation} ({len(batch)} è¡Œ)")

    # 6. æœ€çµ‚æ›´æ–°æ—¥ã‚’æ›´æ–°ï¼ˆè¡Œ1ï¼‰
    now = datetime.now().strftime("%Y-%m-%d %H:%M")
    ws.update_acell("A1", f"æœ€çµ‚æ›´æ–°: {now}")

    logger.info(f"ã‚¹ã‚­ãƒ«ãƒ—ãƒ©ã‚¹ï¼ˆæ—¥åˆ¥ï¼‰æ§‹ç¯‰å®Œäº†: {len(all_rows)} è¡Œ")

    # 7. LINEé€šçŸ¥
    message = (
        f"ğŸ“Š ã‚¹ã‚­ãƒ«ãƒ—ãƒ©ã‚¹ï¼ˆæ—¥åˆ¥ï¼‰ã‚·ãƒ¼ãƒˆæ›´æ–°å®Œäº†\n"
        f"{dates[0]} ã€œ {dates[-1]}\n"
        f"{len(dates)} æ—¥åˆ† / {len(all_rows)} è¡Œ"
    )
    send_line_notify(message)

    return len(all_rows)


# â”€â”€â”€ ã‚¹ã‚­ãƒ«ãƒ—ãƒ©ã‚¹ï¼ˆæœˆåˆ¥ï¼‰æ§‹ç¯‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _format_month(month_key):
    """'2025-07' â†’ '2025å¹´7æœˆ'"""
    y, m = month_key.split("-")
    return f"{y}å¹´{int(m)}æœˆ"


def _calc_kpi_row(month, media, funnel, é›†å®¢æ•°, äºˆç´„æ•°, å®Ÿæ–½æ•°, å£²ä¸Š, åºƒå‘Šè²»):
    """æœˆåˆ¥ã‚·ãƒ¼ãƒˆã®1è¡Œãƒ‡ãƒ¼ã‚¿ã‚’è¨ˆç®—ã—ã¦è¿”ã™"""
    cpa = round(åºƒå‘Šè²» / é›†å®¢æ•°) if é›†å®¢æ•° > 0 else 0
    cpo = round(åºƒå‘Šè²» / äºˆç´„æ•°) if äºˆç´„æ•° > 0 else 0
    roas = round(å£²ä¸Š / åºƒå‘Šè²» * 100, 1) if åºƒå‘Šè²» > 0 else 0
    ltv = round(å£²ä¸Š / é›†å®¢æ•°) if é›†å®¢æ•° > 0 else 0
    ç²—åˆ© = å£²ä¸Š - åºƒå‘Šè²»
    return [_format_month(month), media, funnel, é›†å®¢æ•°, äºˆç´„æ•°, å®Ÿæ–½æ•°, å£²ä¸Š, åºƒå‘Šè²»,
            cpa, cpo, roas, ltv, ç²—åˆ©]


def build_monthly_sheet(dry_run=False):
    """æ—¥åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚’æœˆÃ—é›†å®¢åª’ä½“Ã—ãƒ•ã‚¡ãƒãƒ«åˆ¥ã§é›†è¨ˆã—ã€ã‚¹ã‚­ãƒ«ãƒ—ãƒ©ã‚¹ï¼ˆæœˆåˆ¥ï¼‰ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã‚€"""
    from collections import defaultdict

    # ã‚½ãƒ¼ãƒˆé †å®šç¾©
    CATEGORY_ORDER = ["åºƒå‘Š", "SNS", "SEO", "åºƒå ±", "ãã®ä»–"]
    MEDIA_ORDER = [
        # åºƒå‘Š
        "ãƒªã‚¹ãƒ†ã‚£ãƒ³ã‚°åºƒå‘Š", "ãƒ‡ã‚£ã‚¹ãƒ—ãƒ¬ã‚¤åºƒå‘Š", "YouTubeåºƒå‘Š",
        "Yahoo!åºƒå‘Š", "Yahoo!ãƒªã‚¹ãƒ†ã‚£ãƒ³ã‚°åºƒå‘Š", "Yahoo!ãƒ‡ã‚£ã‚¹ãƒ—ãƒ¬ã‚¤åºƒå‘Š",
        "Metaåºƒå‘Š", "TikTokåºƒå‘Š", "Xåºƒå‘Š", "LINEåºƒå‘Š",
        "ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆåºƒå‘Š", "ã‚ªãƒ•ãƒ©ã‚¤ãƒ³åºƒå‘Š",
        # SNS
        "YouTube", "X", "Instagram", "Threads", "Facebook", "TikTok",
        # SEO
        "ãƒ–ãƒ©ãƒ³ãƒ‰æ¤œç´¢", "ä¸€èˆ¬æ¤œç´¢",
        # åºƒå ±
        "HP", "åºƒå ±",
        # ãã®ä»–
        "note", "ãã®ä»–",
    ]
    FUNNEL_ORDER = [
        "ãƒ–ãƒ©ãƒ³ãƒ‰èªçŸ¥", "ã‚»ãƒ³ã‚µãƒ¼ã‚º", "AI", "ã‚¢ãƒ‰ãƒ—ãƒ­",
        "ç›´å€‹åˆ¥", "ã¿ã‹ã¿ãƒ¡ã‚¤ãƒ³", "ã‚¹ã‚­ãƒ«ãƒ—ãƒ©ã‚¹",
        "ãƒ©ã‚¤ãƒˆãƒ—ãƒ©ãƒ³", "ç§˜å¯†ã®éƒ¨å±‹", "ä¸æ˜",
    ]

    def _cat_idx(cat):
        return CATEGORY_ORDER.index(cat) if cat in CATEGORY_ORDER else len(CATEGORY_ORDER)

    def _media_idx(media):
        return MEDIA_ORDER.index(media) if media in MEDIA_ORDER else len(MEDIA_ORDER)

    def _fun_idx(fun):
        return FUNNEL_ORDER.index(fun) if fun in FUNNEL_ORDER else len(FUNNEL_ORDER)

    # 1. å…¨CSVèª­ã¿è¾¼ã¿
    all_rows = read_all_csvs()
    if not all_rows:
        logger.error("CSVãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return 0

    # 2. (æœˆ, é›†å®¢åª’ä½“, ãƒ•ã‚¡ãƒãƒ«å) ã”ã¨ã«é›†è¨ˆ + åª’ä½“â†’å¤§ã‚«ãƒ†ã‚´ãƒªå¯¾å¿œè¡¨
    detail = defaultdict(lambda: {
        "é›†å®¢æ•°": 0, "äºˆç´„æ•°": 0, "å®Ÿæ–½æ•°": 0, "å£²ä¸Š": 0, "åºƒå‘Šè²»": 0
    })
    media_to_category = {}

    for row in all_rows:
        # row: [æ—¥ä»˜, å¤§ã‚«ãƒ†ã‚´ãƒª, é›†å®¢åª’ä½“, ãƒ•ã‚¡ãƒãƒ«å, é›†å®¢æ•°, äºˆç´„æ•°, å®Ÿæ–½æ•°, å£²ä¸Š, åºƒå‘Šè²», CPA, CPO, ROAS, LTV]
        month_key = row[0][:7]  # "2025-07-01" â†’ "2025-07"
        category = row[1] or "ãã®ä»–"
        media = row[2] or "(æœªåˆ†é¡)"
        funnel = row[3] or "(æœªåˆ†é¡)"
        key = (month_key, media, funnel)
        detail[key]["é›†å®¢æ•°"] += row[4]
        detail[key]["äºˆç´„æ•°"] += row[5]
        detail[key]["å®Ÿæ–½æ•°"] += row[6]
        detail[key]["å£²ä¸Š"] += row[7]
        detail[key]["åºƒå‘Šè²»"] += row[8]
        media_to_category[media] = category

    # 3. æœˆã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ– â†’ è©³ç´°è¡Œ + åˆè¨ˆè¡Œ
    #    ã‚½ãƒ¼ãƒˆ: å¤§ã‚«ãƒ†ã‚´ãƒªé † â†’ åª’ä½“å â†’ ãƒ•ã‚¡ãƒãƒ«é †
    months = sorted(set(k[0] for k in detail.keys()))
    sheet_rows = []
    month_ranges = []  # [(month_key, detail_start_idx, detail_end_idx)]
    month_count = 0

    for month in months:
        month_keys = sorted(
            (k for k in detail.keys() if k[0] == month),
            key=lambda k: (_cat_idx(media_to_category.get(k[1], "ãã®ä»–")),
                           _media_idx(k[1]), k[1], _fun_idx(k[2]), k[2])
        )

        # æœˆåˆè¨ˆã®é›†è¨ˆç”¨
        total = {"é›†å®¢æ•°": 0, "äºˆç´„æ•°": 0, "å®Ÿæ–½æ•°": 0, "å£²ä¸Š": 0, "åºƒå‘Šè²»": 0}
        detail_rows = []

        # è©³ç´°è¡Œ
        for key in month_keys:
            m = detail[key]
            detail_rows.append(_calc_kpi_row(
                month, key[1], key[2],
                m["é›†å®¢æ•°"], m["äºˆç´„æ•°"], m["å®Ÿæ–½æ•°"], m["å£²ä¸Š"], m["åºƒå‘Šè²»"]
            ))
            for k in total:
                total[k] += m[k]

        # åˆè¨ˆè¡Œã‚’å…ˆé ­ã«ã€è©³ç´°è¡Œã‚’ãã®å¾Œã«
        sheet_rows.append(_calc_kpi_row(
            month, "åˆè¨ˆ", "",
            total["é›†å®¢æ•°"], total["äºˆç´„æ•°"], total["å®Ÿæ–½æ•°"], total["å£²ä¸Š"], total["åºƒå‘Šè²»"]
        ))
        detail_start = len(sheet_rows)
        sheet_rows.extend(detail_rows)
        detail_end = len(sheet_rows)
        month_ranges.append((month, detail_start, detail_end))
        month_count += 1

    logger.info(f"æœˆåˆ¥é›†è¨ˆ: {month_count} ãƒ¶æœˆ, {len(sheet_rows)} è¡Œï¼ˆè©³ç´°+åˆè¨ˆï¼‰")
    for r in sheet_rows:
        if r[1] == "åˆè¨ˆ":
            logger.info(f"  {r[0]}: [åˆè¨ˆ] é›†å®¢{r[3]:,} å£²ä¸ŠÂ¥{r[6]:,} åºƒå‘Šè²»Â¥{r[7]:,} ROAS{r[10]}%")

    if dry_run:
        logger.info("(dry-run: æ›¸ãè¾¼ã¿ã‚¹ã‚­ãƒƒãƒ—)")
        return len(sheet_rows)

    # 4. ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã¿
    # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ: è¡Œ1=æœ€çµ‚æ›´æ–°, è¡Œ2=ç©º, è¡Œ3=ãƒ˜ãƒƒãƒ€ãƒ¼, è¡Œ4ã€œ=ãƒ‡ãƒ¼ã‚¿
    client = get_client(ACCOUNT)
    spreadsheet = client.open_by_key(SPREADSHEET_ID)
    ws = spreadsheet.worksheet(MONTHLY_SHEET_NAME)

    # ã‚·ãƒ¼ãƒˆãƒªã‚µã‚¤ã‚ºï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
    needed_rows = 3 + len(sheet_rows) + 100
    current_rows = ws.row_count
    if needed_rows > current_rows:
        ws.resize(rows=needed_rows)
        logger.info(f"ã‚·ãƒ¼ãƒˆãƒªã‚µã‚¤ã‚º: {current_rows} â†’ {needed_rows} è¡Œ")

    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªã‚¢ï¼ˆè¡Œ1ä»¥é™ã™ã¹ã¦ï¼‰
    ws.batch_clear([f"A1:M{max(current_rows, needed_rows)}"])

    # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œ3ã‚’æ›¸ãè¾¼ã¿
    header = ["æœˆ", "é›†å®¢åª’ä½“", "ãƒ•ã‚¡ãƒãƒ«å", "é›†å®¢æ•°", "å€‹åˆ¥äºˆç´„æ•°", "å®Ÿæ–½æ•°",
              "å£²ä¸Š", "åºƒå‘Šè²»", "CPA", "CPO", "ROAS", "LTV", "ç²—åˆ©"]
    ws.update(values=[header], range_name="A3:M3", value_input_option="USER_ENTERED")

    # ãƒ‡ãƒ¼ã‚¿æ›¸ãè¾¼ã¿ï¼ˆ1000è¡Œãšã¤åˆ†å‰²ï¼‰
    BATCH_SIZE = 1000
    for i in range(0, len(sheet_rows), BATCH_SIZE):
        batch = sheet_rows[i:i + BATCH_SIZE]
        start_row = 4 + i
        end_row = start_row + len(batch) - 1
        ws.update(values=batch, range_name=f"A{start_row}:M{end_row}",
                  value_input_option="RAW")
        logger.info(f"æ›¸ãè¾¼ã¿: A{start_row}:M{end_row} ({len(batch)} è¡Œ)")

    # â”€â”€ ä½“è£ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ â”€â”€
    last_row = 3 + len(sheet_rows)

    # æ•°å€¤ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    num_formats = [
        (f"D4:F{last_row}", {"type": "NUMBER", "pattern": "#,##0"}),
        (f"G4:H{last_row}", {"type": "CURRENCY", "pattern": "Â¥#,##0"}),
        (f"I4:J{last_row}", {"type": "CURRENCY", "pattern": "Â¥#,##0"}),
        (f"K4:K{last_row}", {"type": "NUMBER", "pattern": "0.0\"%\""}),
        (f"L4:L{last_row}", {"type": "CURRENCY", "pattern": "Â¥#,##0"}),
        (f"M4:M{last_row}", {"type": "CURRENCY", "pattern": "Â¥#,##0"}),
    ]
    for cell_range, num_fmt in num_formats:
        ws.format(cell_range, {"numberFormat": num_fmt})

    # æœ€çµ‚æ›´æ–°è¡Œï¼ˆè¡Œ1ï¼‰: ã‚°ãƒ¬ãƒ¼æ–‡å­—ãƒ»10pt
    ws.format("A1:M1", {
        "textFormat": {"foregroundColorStyle": {
            "rgbColor": {"red": 0.5, "green": 0.5, "blue": 0.5}
        }, "fontSize": 10},
    })

    # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œï¼ˆè¡Œ3ï¼‰: å¤ªå­—ãƒ»èƒŒæ™¯è‰²ãƒ»ç™½æ–‡å­—ãƒ»ä¸­å¤®æƒãˆ
    ws.format("A3:M3", {
        "textFormat": {"bold": True, "foregroundColorStyle": {
            "rgbColor": {"red": 1, "green": 1, "blue": 1}
        }},
        "backgroundColor": {"red": 0.2, "green": 0.4, "blue": 0.65},
        "horizontalAlignment": "CENTER",
    })

    # åˆè¨ˆè¡Œ: å¤ªå­—ãƒ»è–„ã„èƒŒæ™¯è‰²
    total_row_indices = [i for i, r in enumerate(sheet_rows) if r[1] == "åˆè¨ˆ"]
    for idx in total_row_indices:
        row_num = 4 + idx
        ws.format(f"A{row_num}:M{row_num}", {
            "textFormat": {"bold": True},
            "backgroundColor": {"red": 0.9, "green": 0.93, "blue": 0.98},
        })

    # â”€â”€ ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼ˆéå»æœˆã®è©³ç´°è¡Œã‚’æŠ˜ã‚ŠãŸãŸã¿ï¼‰ â”€â”€
    # æ—¢å­˜ã‚°ãƒ«ãƒ¼ãƒ—ã‚’å‰Šé™¤
    try:
        meta = spreadsheet.fetch_sheet_metadata()
        del_reqs = []
        for s in meta.get("sheets", []):
            if s["properties"]["sheetId"] == ws.id:
                for g in s.get("rowGroups", []):
                    del_reqs.append({"deleteDimensionGroup": {"range": g["range"]}})
                break
        if del_reqs:
            spreadsheet.batch_update({"requests": del_reqs})
            logger.info(f"æ—¢å­˜ã‚°ãƒ«ãƒ¼ãƒ—å‰Šé™¤: {len(del_reqs)} ä»¶")
    except Exception as e:
        logger.warning(f"æ—¢å­˜ã‚°ãƒ«ãƒ¼ãƒ—å‰Šé™¤ã‚¹ã‚­ãƒƒãƒ—: {e}")

    # ãƒ•ãƒªãƒ¼ã‚º + ã‚°ãƒ«ãƒ¼ãƒ—ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ä½ç½®ï¼ˆåˆè¨ˆè¡Œã®ä¸Šã«+/-ãƒœã‚¿ãƒ³ï¼‰ + ã‚°ãƒ«ãƒ¼ãƒ—è¿½åŠ 
    current_month = datetime.now().strftime("%Y-%m")
    requests = [{
        "updateSheetProperties": {
            "properties": {
                "sheetId": ws.id,
                "gridProperties": {
                    "frozenRowCount": 3,
                    "rowGroupControlAfter": False,
                },
            },
            "fields": "gridProperties.frozenRowCount,gridProperties.rowGroupControlAfter",
        }
    }]

    for month_key, detail_start, detail_end in month_ranges:
        if detail_start >= detail_end:
            continue
        requests.append({
            "addDimensionGroup": {
                "range": {
                    "sheetId": ws.id,
                    "dimension": "ROWS",
                    "startIndex": 3 + detail_start,
                    "endIndex": 3 + detail_end,
                }
            }
        })

    spreadsheet.batch_update({"requests": requests})

    # éå»æœˆã®ã‚°ãƒ«ãƒ¼ãƒ—ã‚’æŠ˜ã‚ŠãŸãŸã¿
    collapse_reqs = []
    for month_key, detail_start, detail_end in month_ranges:
        if detail_start >= detail_end or month_key >= current_month:
            continue
        collapse_reqs.append({
            "updateDimensionGroup": {
                "dimensionGroup": {
                    "range": {
                        "sheetId": ws.id,
                        "dimension": "ROWS",
                        "startIndex": 3 + detail_start,
                        "endIndex": 3 + detail_end,
                    },
                    "depth": 1,
                    "collapsed": True,
                },
                "fields": "collapsed",
            }
        })

    if collapse_reqs:
        spreadsheet.batch_update({"requests": collapse_reqs})
        logger.info(f"éå»æœˆã‚°ãƒ«ãƒ¼ãƒ—æŠ˜ã‚ŠãŸãŸã¿: {len(collapse_reqs)} ãƒ¶æœˆ")

    # â”€â”€ ãƒãƒ£ãƒ¼ãƒˆç”¨ã‚µãƒãƒªãƒ¼ãƒ†ãƒ¼ãƒ–ãƒ« â”€â”€
    summary_start = last_row + 3  # 1-indexed
    total_rows_data = []
    for r in sheet_rows:
        if r[1] == "åˆè¨ˆ":
            # [æœˆ, é›†å®¢æ•°, äºˆç´„æ•°, å®Ÿæ–½æ•°, å£²ä¸Š, åºƒå‘Šè²», ROAS, ç²—åˆ©]
            total_rows_data.append([r[0], r[3], r[4], r[5], r[6], r[7], r[10], r[12]])

    summary_header = ["æœˆ", "é›†å®¢æ•°", "å€‹åˆ¥äºˆç´„æ•°", "å®Ÿæ–½æ•°", "å£²ä¸Š", "åºƒå‘Šè²»", "ROAS(%)", "ç²—åˆ©"]
    all_summary = [summary_header] + total_rows_data
    summary_end = summary_start + len(all_summary) - 1

    ws.update(
        values=all_summary,
        range_name=f"A{summary_start}:H{summary_end}",
        value_input_option="RAW",
    )
    # ã‚µãƒãƒªãƒ¼ãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    ws.format(f"A{summary_start}:H{summary_start}", {
        "textFormat": {"bold": True},
        "backgroundColor": {"red": 0.85, "green": 0.85, "blue": 0.85},
    })
    ws.format(f"E{summary_start + 1}:F{summary_end}", {
        "numberFormat": {"type": "CURRENCY", "pattern": "Â¥#,##0"},
    })
    ws.format(f"H{summary_start + 1}:H{summary_end}", {
        "numberFormat": {"type": "CURRENCY", "pattern": "Â¥#,##0"},
    })
    ws.format(f"B{summary_start + 1}:D{summary_end}", {
        "numberFormat": {"type": "NUMBER", "pattern": "#,##0"},
    })
    ws.format(f"G{summary_start + 1}:G{summary_end}", {
        "numberFormat": {"type": "NUMBER", "pattern": "0.0\"%\""},
    })
    logger.info(f"ã‚µãƒãƒªãƒ¼ãƒ†ãƒ¼ãƒ–ãƒ«: A{summary_start}:H{summary_end}")

    # â”€â”€ ãƒãƒ£ãƒ¼ãƒˆä½œæˆ â”€â”€
    # æ—¢å­˜ãƒãƒ£ãƒ¼ãƒˆã‚’å‰Šé™¤
    try:
        meta = spreadsheet.fetch_sheet_metadata()
        chart_del = []
        for s in meta.get("sheets", []):
            if s["properties"]["sheetId"] == ws.id:
                for chart in s.get("charts", []):
                    chart_del.append({"deleteEmbeddedObject": {"objectId": chart["chartId"]}})
                break
        if chart_del:
            spreadsheet.batch_update({"requests": chart_del})
            logger.info(f"æ—¢å­˜ãƒãƒ£ãƒ¼ãƒˆå‰Šé™¤: {len(chart_del)} ä»¶")
    except Exception as e:
        logger.warning(f"æ—¢å­˜ãƒãƒ£ãƒ¼ãƒˆå‰Šé™¤ã‚¹ã‚­ãƒƒãƒ—: {e}")

    # ãƒãƒ£ãƒ¼ãƒˆç”¨ã‚½ãƒ¼ã‚¹ç¯„å›²ãƒ˜ãƒ«ãƒ‘ãƒ¼ï¼ˆ0-indexedï¼‰
    sr0 = summary_start - 1  # 0-indexed header row
    sr1 = summary_end        # 0-indexed exclusive

    def _src(col_start, col_end):
        return {"sourceRange": {"sources": [{
            "sheetId": ws.id,
            "startRowIndex": sr0, "endRowIndex": sr1,
            "startColumnIndex": col_start, "endColumnIndex": col_end,
        }]}}

    chart_row = summary_end + 1  # 0-indexed anchor row for charts

    chart_reqs = [
        # Chart 1: å£²ä¸Šãƒ»åºƒå‘Šè²»ãƒ»ç²—åˆ©ï¼ˆæ£’ã‚°ãƒ©ãƒ•ï¼‰
        {"addChart": {"chart": {
            "spec": {
                "title": "å£²ä¸Šãƒ»åºƒå‘Šè²»ãƒ»ç²—åˆ©",
                "basicChart": {
                    "chartType": "COLUMN",
                    "legendPosition": "BOTTOM_LEGEND",
                    "axis": [
                        {"position": "BOTTOM_AXIS"},
                        {"position": "LEFT_AXIS", "title": "é‡‘é¡"},
                    ],
                    "domains": [{"domain": _src(0, 1)}],
                    "series": [
                        {"series": _src(4, 5), "targetAxis": "LEFT_AXIS",
                         "color": {"red": 0.27, "green": 0.45, "blue": 0.77}},
                        {"series": _src(5, 6), "targetAxis": "LEFT_AXIS",
                         "color": {"red": 0.75, "green": 0.31, "blue": 0.30}},
                        {"series": _src(7, 8), "targetAxis": "LEFT_AXIS",
                         "color": {"red": 0.44, "green": 0.68, "blue": 0.28}},
                    ],
                    "headerCount": 1,
                },
            },
            "position": {"overlayPosition": {
                "anchorCell": {"sheetId": ws.id, "rowIndex": chart_row, "columnIndex": 0},
                "widthPixels": 720, "heightPixels": 400,
            }},
        }}},
        # Chart 2: ROASæ¨ç§»ï¼ˆæŠ˜ã‚Œç·šã‚°ãƒ©ãƒ•ï¼‰
        {"addChart": {"chart": {
            "spec": {
                "title": "ROASæ¨ç§»",
                "basicChart": {
                    "chartType": "LINE",
                    "legendPosition": "BOTTOM_LEGEND",
                    "axis": [
                        {"position": "BOTTOM_AXIS"},
                        {"position": "LEFT_AXIS", "title": "ROAS (%)"},
                    ],
                    "domains": [{"domain": _src(0, 1)}],
                    "series": [
                        {"series": _src(6, 7), "targetAxis": "LEFT_AXIS",
                         "color": {"red": 0.93, "green": 0.49, "blue": 0.19}},
                    ],
                    "headerCount": 1,
                },
            },
            "position": {"overlayPosition": {
                "anchorCell": {"sheetId": ws.id, "rowIndex": chart_row, "columnIndex": 7},
                "widthPixels": 520, "heightPixels": 400,
            }},
        }}},
        # Chart 3: é›†å®¢æ•°ãƒ»äºˆç´„æ•°ãƒ»å®Ÿæ–½æ•°ï¼ˆæ£’ã‚°ãƒ©ãƒ•ï¼‰
        {"addChart": {"chart": {
            "spec": {
                "title": "é›†å®¢æ•°ãƒ»äºˆç´„æ•°ãƒ»å®Ÿæ–½æ•°",
                "basicChart": {
                    "chartType": "COLUMN",
                    "legendPosition": "BOTTOM_LEGEND",
                    "axis": [
                        {"position": "BOTTOM_AXIS"},
                        {"position": "LEFT_AXIS", "title": "ä»¶æ•°"},
                    ],
                    "domains": [{"domain": _src(0, 1)}],
                    "series": [
                        {"series": _src(1, 2), "targetAxis": "LEFT_AXIS",
                         "color": {"red": 0.27, "green": 0.45, "blue": 0.77}},
                        {"series": _src(2, 3), "targetAxis": "LEFT_AXIS",
                         "color": {"red": 0.44, "green": 0.68, "blue": 0.28}},
                        {"series": _src(3, 4), "targetAxis": "LEFT_AXIS",
                         "color": {"red": 0.93, "green": 0.49, "blue": 0.19}},
                    ],
                    "headerCount": 1,
                },
            },
            "position": {"overlayPosition": {
                "anchorCell": {"sheetId": ws.id, "rowIndex": chart_row + 23, "columnIndex": 0},
                "widthPixels": 720, "heightPixels": 400,
            }},
        }}},
    ]
    spreadsheet.batch_update({"requests": chart_reqs})
    logger.info("ãƒãƒ£ãƒ¼ãƒˆä½œæˆ: 3 ä»¶")

    # æœ€çµ‚æ›´æ–°æ—¥
    now = datetime.now().strftime("%Y-%m-%d %H:%M")
    ws.update_acell("A1", f"æœ€çµ‚æ›´æ–°: {now}")

    logger.info(f"ã‚¹ã‚­ãƒ«ãƒ—ãƒ©ã‚¹ï¼ˆæœˆåˆ¥ï¼‰æ§‹ç¯‰å®Œäº†: {month_count} ãƒ¶æœˆ, {len(sheet_rows)} è¡Œ")

    # LINEé€šçŸ¥
    message = (
        f"ğŸ“Š ã‚¹ã‚­ãƒ«ãƒ—ãƒ©ã‚¹ï¼ˆæœˆåˆ¥ï¼‰ã‚·ãƒ¼ãƒˆæ›´æ–°å®Œäº†\n"
        f"{months[0]} ã€œ {months[-1]} ({month_count} ãƒ¶æœˆ, {len(sheet_rows)} è¡Œ)"
    )
    send_line_notify(message)

    return len(sheet_rows)


# â”€â”€â”€ KPIã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”Ÿæˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def generate_kpi_cache(dry_run=False):
    """å…¨CSVãƒ‡ãƒ¼ã‚¿ã‹ã‚‰KPIã‚µãƒãƒªãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆJSONï¼‰ã‚’ç”Ÿæˆã™ã‚‹ã€‚
    AIç§˜æ›¸ãŒã‚·ãƒ¼ãƒˆå‚ç…§ãªã—ã§å³åº§ã«KPIã‚’å›ç­”ã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ã€‚"""
    from collections import defaultdict

    all_rows = read_all_csvs()
    if not all_rows:
        logger.error("CSVãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ â†’ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”Ÿæˆã‚¹ã‚­ãƒƒãƒ—")
        return False

    # â”€â”€ 1. æœˆåˆ¥ã‚µãƒãƒªï¼ˆå…¨ä½“ï¼‰ â”€â”€
    monthly = defaultdict(lambda: {"é›†å®¢æ•°": 0, "äºˆç´„æ•°": 0, "å®Ÿæ–½æ•°": 0, "å£²ä¸Š": 0, "åºƒå‘Šè²»": 0})
    for row in all_rows:
        mk = row[0][:7]
        monthly[mk]["é›†å®¢æ•°"] += row[4]
        monthly[mk]["äºˆç´„æ•°"] += row[5]
        monthly[mk]["å®Ÿæ–½æ•°"] += row[6]
        monthly[mk]["å£²ä¸Š"] += row[7]
        monthly[mk]["åºƒå‘Šè²»"] += row[8]

    monthly_list = []
    for month in sorted(monthly.keys()):
        m = monthly[month]
        é›†å®¢ = m["é›†å®¢æ•°"]; äºˆç´„ = m["äºˆç´„æ•°"]; å®Ÿæ–½ = m["å®Ÿæ–½æ•°"]
        å£²ä¸Š = m["å£²ä¸Š"]; åºƒå‘Šè²» = m["åºƒå‘Šè²»"]
        monthly_list.append({
            "month": month,
            "é›†å®¢æ•°": é›†å®¢, "å€‹åˆ¥äºˆç´„æ•°": äºˆç´„, "å®Ÿæ–½æ•°": å®Ÿæ–½,
            "å£²ä¸Š": å£²ä¸Š, "åºƒå‘Šè²»": åºƒå‘Šè²»,
            "CPA": round(åºƒå‘Šè²» / é›†å®¢) if é›†å®¢ > 0 else 0,
            "CPO": round(åºƒå‘Šè²» / äºˆç´„) if äºˆç´„ > 0 else 0,
            "ROAS": round(å£²ä¸Š / åºƒå‘Šè²» * 100, 1) if åºƒå‘Šè²» > 0 else 0,
            "LTV": round(å£²ä¸Š / é›†å®¢) if é›†å®¢ > 0 else 0,
            "ç²—åˆ©": å£²ä¸Š - åºƒå‘Šè²»,
        })

    # â”€â”€ 2. æœˆåˆ¥Ã—åª’ä½“ å†…è¨³ â”€â”€
    media_monthly = defaultdict(lambda: defaultdict(lambda: {"é›†å®¢æ•°": 0, "äºˆç´„æ•°": 0, "å£²ä¸Š": 0, "åºƒå‘Šè²»": 0}))
    for row in all_rows:
        mk = row[0][:7]
        media = row[2]  # é›†å®¢åª’ä½“
        if not media:
            continue
        media_monthly[mk][media]["é›†å®¢æ•°"] += row[4]
        media_monthly[mk][media]["äºˆç´„æ•°"] += row[5]
        media_monthly[mk][media]["å£²ä¸Š"] += row[7]
        media_monthly[mk][media]["åºƒå‘Šè²»"] += row[8]

    monthly_by_media = {}
    for mk in sorted(media_monthly.keys()):
        monthly_by_media[mk] = {}
        for media, vals in sorted(media_monthly[mk].items()):
            monthly_by_media[mk][media] = {
                "é›†å®¢æ•°": vals["é›†å®¢æ•°"], "å€‹åˆ¥äºˆç´„æ•°": vals["äºˆç´„æ•°"],
                "å£²ä¸Š": vals["å£²ä¸Š"], "åºƒå‘Šè²»": vals["åºƒå‘Šè²»"],
                "ROAS": round(vals["å£²ä¸Š"] / vals["åºƒå‘Šè²»"] * 100, 1) if vals["åºƒå‘Šè²»"] > 0 else 0,
            }

    # â”€â”€ 2b. æœˆåˆ¥Ã—åª’ä½“Ã—ãƒ•ã‚¡ãƒãƒ« å†…è¨³ â”€â”€
    mf_monthly = defaultdict(lambda: defaultdict(lambda: {
        "é›†å®¢æ•°": 0, "äºˆç´„æ•°": 0, "å®Ÿæ–½æ•°": 0, "å£²ä¸Š": 0, "åºƒå‘Šè²»": 0
    }))
    for row in all_rows:
        mk = row[0][:7]
        media = row[2] or "(æœªåˆ†é¡)"
        funnel = row[3] or "(æœªåˆ†é¡)"
        mf_key = f"{media}|{funnel}"
        mf_monthly[mk][mf_key]["é›†å®¢æ•°"] += row[4]
        mf_monthly[mk][mf_key]["äºˆç´„æ•°"] += row[5]
        mf_monthly[mk][mf_key]["å®Ÿæ–½æ•°"] += row[6]
        mf_monthly[mk][mf_key]["å£²ä¸Š"] += row[7]
        mf_monthly[mk][mf_key]["åºƒå‘Šè²»"] += row[8]

    monthly_by_media_funnel = {}
    for mk in sorted(mf_monthly.keys()):
        monthly_by_media_funnel[mk] = {}
        for mf_key, vals in sorted(mf_monthly[mk].items()):
            é›†å®¢ = vals["é›†å®¢æ•°"]; äºˆç´„ = vals["äºˆç´„æ•°"]
            å£²ä¸Š = vals["å£²ä¸Š"]; åºƒå‘Šè²» = vals["åºƒå‘Šè²»"]
            monthly_by_media_funnel[mk][mf_key] = {
                "é›†å®¢åª’ä½“": mf_key.split("|")[0],
                "ãƒ•ã‚¡ãƒãƒ«å": mf_key.split("|")[1],
                "é›†å®¢æ•°": é›†å®¢, "å€‹åˆ¥äºˆç´„æ•°": äºˆç´„,
                "å®Ÿæ–½æ•°": vals["å®Ÿæ–½æ•°"], "å£²ä¸Š": å£²ä¸Š, "åºƒå‘Šè²»": åºƒå‘Šè²»,
                "CPA": round(åºƒå‘Šè²» / é›†å®¢) if é›†å®¢ > 0 else 0,
                "CPO": round(åºƒå‘Šè²» / äºˆç´„) if äºˆç´„ > 0 else 0,
                "ROAS": round(å£²ä¸Š / åºƒå‘Šè²» * 100, 1) if åºƒå‘Šè²» > 0 else 0,
                "LTV": round(å£²ä¸Š / é›†å®¢) if é›†å®¢ > 0 else 0,
                "ç²—åˆ©": å£²ä¸Š - åºƒå‘Šè²»,
            }

    # â”€â”€ 3. ç›´è¿‘14æ—¥ æ—¥åˆ¥åˆè¨ˆ â”€â”€
    daily_totals = defaultdict(lambda: {"é›†å®¢æ•°": 0, "äºˆç´„æ•°": 0, "å£²ä¸Š": 0, "åºƒå‘Šè²»": 0})
    for row in all_rows:
        dt = row[0]
        daily_totals[dt]["é›†å®¢æ•°"] += row[4]
        daily_totals[dt]["äºˆç´„æ•°"] += row[5]
        daily_totals[dt]["å£²ä¸Š"] += row[7]
        daily_totals[dt]["åºƒå‘Šè²»"] += row[8]

    sorted_dates = sorted(daily_totals.keys(), reverse=True)[:14]
    recent_daily = []
    for dt in sorted_dates:
        d = daily_totals[dt]
        recent_daily.append({
            "date": dt,
            "é›†å®¢æ•°": d["é›†å®¢æ•°"], "å€‹åˆ¥äºˆç´„æ•°": d["äºˆç´„æ•°"],
            "å£²ä¸Š": d["å£²ä¸Š"], "åºƒå‘Šè²»": d["åºƒå‘Šè²»"],
            "ROAS": round(d["å£²ä¸Š"] / d["åºƒå‘Šè²»"] * 100, 1) if d["åºƒå‘Šè²»"] > 0 else 0,
        })

    # â”€â”€ 4. ç›´è¿‘14æ—¥ æ—¥åˆ¥Ã—åª’ä½“ â”€â”€
    media_daily = defaultdict(lambda: defaultdict(lambda: {"é›†å®¢æ•°": 0, "å£²ä¸Š": 0, "åºƒå‘Šè²»": 0}))
    for row in all_rows:
        dt = row[0]
        if dt not in sorted_dates:
            continue
        media = row[2]
        if not media:
            continue
        media_daily[dt][media]["é›†å®¢æ•°"] += row[4]
        media_daily[dt][media]["å£²ä¸Š"] += row[7]
        media_daily[dt][media]["åºƒå‘Šè²»"] += row[8]

    recent_daily_by_media = {}
    for dt in sorted_dates:
        recent_daily_by_media[dt] = {}
        for media, vals in sorted(media_daily[dt].items()):
            recent_daily_by_media[dt][media] = {
                "é›†å®¢æ•°": vals["é›†å®¢æ•°"], "å£²ä¸Š": vals["å£²ä¸Š"], "åºƒå‘Šè²»": vals["åºƒå‘Šè²»"],
            }

    # â”€â”€ 5. JSONå‡ºåŠ› â”€â”€
    cache = {
        "updated_at": datetime.now().isoformat(timespec="seconds"),
        "monthly": monthly_list,
        "monthly_by_media": monthly_by_media,
        "monthly_by_media_funnel": monthly_by_media_funnel,
        "recent_daily": recent_daily,
        "recent_daily_by_media": recent_daily_by_media,
    }

    # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³: ç©ºãƒ‡ãƒ¼ã‚¿ãƒã‚§ãƒƒã‚¯
    if not monthly_list and not recent_daily:
        logger.warning("KPIã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”Ÿæˆã‚¹ã‚­ãƒƒãƒ—: æœˆåˆ¥ãƒ»æ—¥åˆ¥ãƒ‡ãƒ¼ã‚¿ãŒä¸¡æ–¹ç©ºã§ã™")
        return False

    if dry_run:
        logger.info(f"(dry-run) KPIã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”Ÿæˆäºˆå®š: {len(monthly_list)}ãƒ¶æœˆ, {len(recent_daily)}æ—¥åˆ†")
        return True

    # ã‚¢ãƒˆãƒŸãƒƒã‚¯æ›¸ãè¾¼ã¿: tmpfile â†’ rename ã§ç ´æã‚’é˜²æ­¢
    KPI_CACHE_PATH.parent.mkdir(parents=True, exist_ok=True)
    json_str = json.dumps(cache, ensure_ascii=False, indent=2)
    fd, tmp_path = tempfile.mkstemp(dir=KPI_CACHE_PATH.parent, suffix=".tmp")
    try:
        with os.fdopen(fd, "w", encoding="utf-8") as f:
            f.write(json_str)
        os.replace(tmp_path, str(KPI_CACHE_PATH))
    except Exception:
        if os.path.exists(tmp_path):
            os.unlink(tmp_path)
        raise

    logger.info(f"KPIã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”Ÿæˆå®Œäº†: {KPI_CACHE_PATH} ({len(monthly_list)}ãƒ¶æœˆ, {len(recent_daily)}æ—¥åˆ†)")
    return True


# â”€â”€â”€ CLI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if __name__ == "__main__":
    args = sys.argv[1:]
    dry_run = "--dry-run" in args

    try:
        if "cache" in args:
            # KPIã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã¿å†ç”Ÿæˆ
            generate_kpi_cache(dry_run=dry_run)
        elif "build" in args:
            # ã‚¹ã‚­ãƒ«ãƒ—ãƒ©ã‚¹ï¼ˆæ—¥åˆ¥ï¼‰ã‚·ãƒ¼ãƒˆã®ã¿æ§‹ç¯‰
            count = build_daily_sheet(dry_run=dry_run)
            if count > 0:
                logger.info(f"å®Œäº†: {count} è¡Œæ›¸ãè¾¼ã¿")
                generate_kpi_cache(dry_run=dry_run)
        elif "monthly" in args:
            # ã‚¹ã‚­ãƒ«ãƒ—ãƒ©ã‚¹ï¼ˆæœˆåˆ¥ï¼‰ã‚·ãƒ¼ãƒˆã®ã¿æ§‹ç¯‰
            count = build_monthly_sheet(dry_run=dry_run)
            if count > 0:
                logger.info(f"å®Œäº†: {count} ãƒ¶æœˆåˆ†æ›¸ãè¾¼ã¿")
                generate_kpi_cache(dry_run=dry_run)
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: å…ƒãƒ‡ãƒ¼ã‚¿ â†’ æ—¥åˆ¥ â†’ æœˆåˆ¥ ã®é€£é–å®Ÿè¡Œ
            count = sync_to_sheet(dry_run=dry_run)
            if count > 0:
                logger.info(f"å…ƒãƒ‡ãƒ¼ã‚¿: {count} è¡Œæ›´æ–° â†’ æ—¥åˆ¥ãƒ»æœˆåˆ¥ã‚’å†æ§‹ç¯‰")
                daily = build_daily_sheet(dry_run=dry_run)
                logger.info(f"æ—¥åˆ¥: {daily} è¡Œæ›¸ãè¾¼ã¿ â†’ æœˆåˆ¥ã‚’å†é›†è¨ˆ")
                monthly = build_monthly_sheet(dry_run=dry_run)
                logger.info(f"æœˆåˆ¥: {monthly} ãƒ¶æœˆåˆ†æ›¸ãè¾¼ã¿ â†’ KPIã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”Ÿæˆ")
                generate_kpi_cache(dry_run=dry_run)
            else:
                logger.info("å…ƒãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›´ãªã— â†’ æ—¥åˆ¥ãƒ»æœˆåˆ¥ã®æ›´æ–°ã‚¹ã‚­ãƒƒãƒ—")
        sys.exit(0)
    except Exception as e:
        logger.error(f"ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        sys.exit(1)
