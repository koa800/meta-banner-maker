#!/usr/bin/env python3
"""
CSV â†’ Google Sheets è‡ªå‹•åŒæœŸã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ã€ŒLooker Studio CSVã€ãƒ•ã‚©ãƒ«ãƒ€å†…ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡ºã—ã€
ã€Œå…ƒãƒ‡ãƒ¼ã‚¿ã€ã‚·ãƒ¼ãƒˆã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’è‡ªå‹•æ›´æ–°ã™ã‚‹ã€‚

å‹•ä½œ:
  1. ãƒ•ã‚©ãƒ«ãƒ€å†…ã®æ—¥ä»˜ä»˜ãCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³
  2. ã‚·ãƒ¼ãƒˆã®ã€Œè¦ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã€è¡Œã¨ãƒãƒƒãƒãƒ³ã‚°
  3. ãƒãƒƒãƒã—ãŸè¡Œã®æŠ•å…¥æ—¥æ™‚ãƒ»ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ã€Œå®Œäº†ã€ã«æ›´æ–°
  4. æ—¥ä»˜ä¸æ˜ã®CSVï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåï¼‰ã‚’æ¤œå‡ºã—ãŸã‚‰LINEã§é€šçŸ¥

ä½¿ã„æ–¹:
  python3 csv_sheet_sync.py              # å…ƒãƒ‡ãƒ¼ã‚¿ã‚·ãƒ¼ãƒˆåŒæœŸ
  python3 csv_sheet_sync.py --dry-run    # ç¢ºèªã®ã¿ï¼ˆæ›¸ãè¾¼ã¿ã—ãªã„ï¼‰
  python3 csv_sheet_sync.py build        # ã‚¹ã‚­ãƒ«ãƒ—ãƒ©ã‚¹ï¼ˆæ—¥åˆ¥ï¼‰ã‚·ãƒ¼ãƒˆã‚’å…¨CSV ã‹ã‚‰æ§‹ç¯‰
  python3 csv_sheet_sync.py build --dry-run
  python3 csv_sheet_sync.py monthly      # ã‚¹ã‚­ãƒ«ãƒ—ãƒ©ã‚¹ï¼ˆæœˆåˆ¥ï¼‰ã‚·ãƒ¼ãƒˆã‚’æ—¥åˆ¥ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é›†è¨ˆ
  python3 csv_sheet_sync.py monthly --dry-run
  python3 csv_sheet_sync.py cache        # KPIã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã¿å†ç”Ÿæˆ
"""

import os
import sys
import re
import csv
import json
import logging
import requests
from datetime import datetime

# sheets_manager ã¨åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, BASE_DIR)
from sheets_manager import get_client, extract_spreadsheet_id

# â”€â”€â”€ è¨­å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CSV_DIR = os.path.expanduser("~/Desktop/Looker Studio CSV")
SPREADSHEET_ID = "1FOh_XGZWaEisfFEngiN848kSm2E6HotAZiMDTmO7BNA"
SPREADSHEET_URL = f"https://docs.google.com/spreadsheets/d/{SPREADSHEET_ID}/edit?gid=1948910703"
SHEET_NAME = "å…ƒãƒ‡ãƒ¼ã‚¿"
DAILY_SHEET_NAME = "ã‚¹ã‚­ãƒ«ãƒ—ãƒ©ã‚¹ï¼ˆæ—¥åˆ¥ï¼‰"
MONTHLY_SHEET_NAME = "ã‚¹ã‚­ãƒ«ãƒ—ãƒ©ã‚¹ï¼ˆæœˆåˆ¥ï¼‰"

# KPIã‚µãƒãƒªãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥
KPI_CACHE_PATH = os.path.join(BASE_DIR, "data", "kpi_summary.json")
ACCOUNT = "kohara"
BASE_CSV_NAME = "ã‚¢ãƒ‰ãƒã‚¹å…¨ä½“æ•°å€¤_åª’ä½“ãƒ»ãƒ•ã‚¡ãƒãƒ«åˆ¥ãƒ‡ãƒ¼ã‚¿_è¡¨"

# LINEé€šçŸ¥è¨­å®š
CONFIG_PATH = os.path.join(BASE_DIR, "line_bot_local", "config.json")
SERVER_URL = "https://line-mention-bot-mmzu.onrender.com"
AGENT_TOKEN = ""
if os.path.exists(CONFIG_PATH):
    with open(CONFIG_PATH) as f:
        _cfg = json.load(f)
        AGENT_TOKEN = _cfg.get("agent_token", "")

# é€šçŸ¥æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®è¨˜éŒ²ï¼ˆåŒã˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½•åº¦ã‚‚é€šçŸ¥ã—ãªã„ï¼‰
NOTIFIED_FILE = os.path.join(BASE_DIR, "csv_sheet_sync_notified.json")

# ãƒ­ã‚°è¨­å®š
LOG_FILE = os.path.join(BASE_DIR, "csv_sheet_sync.log")
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(LOG_FILE, encoding="utf-8"),
        logging.StreamHandler(),
    ],
)
logger = logging.getLogger(__name__)

# æ—¥ä»˜ä»˜ããƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³: 2025-07-01_ã‚¢ãƒ‰ãƒã‚¹å…¨ä½“æ•°å€¤_...csv
DATE_PATTERN = re.compile(r'^(\d{4}-\d{2}-\d{2})_(.+)\.csv$')


# â”€â”€â”€ LINEé€šçŸ¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def send_line_notify(message: str) -> bool:
    """LINEç§˜æ›¸ã‚°ãƒ«ãƒ¼ãƒ—ã«é€šçŸ¥ã‚’é€ã‚‹"""
    if not AGENT_TOKEN:
        logger.warning("AGENT_TOKENæœªè¨­å®š: LINEé€šçŸ¥ã‚’ã‚¹ã‚­ãƒƒãƒ—")
        return False
    try:
        resp = requests.post(
            f"{SERVER_URL}/notify",
            headers={"Authorization": f"Bearer {AGENT_TOKEN}"},
            json={"message": message},
            timeout=40,
        )
        if resp.status_code == 200:
            logger.info("LINEé€šçŸ¥é€ä¿¡å®Œäº†")
            return True
        else:
            logger.error(f"LINEé€šçŸ¥å¤±æ•—: {resp.status_code} {resp.text[:200]}")
            return False
    except Exception as e:
        logger.error(f"LINEé€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def load_notified():
    """é€šçŸ¥æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚€"""
    if os.path.exists(NOTIFIED_FILE):
        with open(NOTIFIED_FILE) as f:
            return set(json.load(f))
    return set()


def save_notified(notified: set):
    """é€šçŸ¥æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’ä¿å­˜ã™ã‚‹"""
    with open(NOTIFIED_FILE, "w") as f:
        json.dump(sorted(notified), f, ensure_ascii=False)


# â”€â”€â”€ ã‚¹ã‚­ãƒ£ãƒ³ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def scan_csv_folder():
    """ãƒ•ã‚©ãƒ«ãƒ€å†…ã®æ—¥ä»˜ä»˜ãCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã¦ {æ—¥ä»˜: ãƒ•ã‚¡ã‚¤ãƒ«å} ã‚’è¿”ã™"""
    if not os.path.isdir(CSV_DIR):
        logger.error(f"ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {CSV_DIR}")
        return {}, []

    dated_files = {}
    unnamed_files = []

    for f in os.listdir(CSV_DIR):
        if not f.endswith(".csv") or BASE_CSV_NAME not in f:
            continue

        m = DATE_PATTERN.match(f)
        if m:
            dated_files[m.group(1)] = f
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåï¼ˆæ—¥ä»˜ãªã—ï¼‰ã®CSV
            unnamed_files.append(f)

    return dated_files, unnamed_files


def check_unnamed_files(unnamed_files: list, dry_run: bool = False):
    """æ—¥ä»˜ä¸æ˜ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡ºã—ã¦LINEã§é€šçŸ¥ã™ã‚‹"""
    if not unnamed_files:
        return

    # é€šçŸ¥æ¸ˆã¿ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿ã€æœªé€šçŸ¥ã®ãƒ•ã‚¡ã‚¤ãƒ«ã ã‘æŠ½å‡º
    notified = load_notified()
    new_unnamed = [f for f in unnamed_files if f not in notified]

    if not new_unnamed:
        return

    logger.info(f"æ—¥ä»˜ä¸æ˜ã®CSVãƒ•ã‚¡ã‚¤ãƒ«: {len(new_unnamed)} ä»¶")
    for f in new_unnamed:
        logger.info(f"  - {f}")

    if dry_run:
        logger.info("(dry-run: LINEé€šçŸ¥ã‚¹ã‚­ãƒƒãƒ—)")
        return

    # LINEé€šçŸ¥
    file_list = "\n".join(f"ãƒ»{f}" for f in new_unnamed[:10])
    if len(new_unnamed) > 10:
        file_list += f"\n... ä»– {len(new_unnamed) - 10} ä»¶"

    message = (
        f"ğŸ“Š Looker Studio CSV: æ—¥ä»˜ä¸æ˜ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒ {len(new_unnamed)} ä»¶ã‚ã‚Šã¾ã™\n\n"
        f"{file_list}\n\n"
        f"ãƒ•ã‚¡ã‚¤ãƒ«åã®å…ˆé ­ã«æ—¥ä»˜ã‚’ä»˜ã‘ã¦ãã ã•ã„ã€‚\n"
        f"ä¾‹: 2026-02-20_{BASE_CSV_NAME}.csv\n\n"
        f"æ—¥ä»˜ã‚’ä»˜ã‘ã‚‹ã¨è‡ªå‹•ã§ã‚·ãƒ¼ãƒˆã«åæ˜ ã•ã‚Œã¾ã™ã€‚"
    )
    send_line_notify(message)

    # é€šçŸ¥æ¸ˆã¿ã«è¿½åŠ 
    notified.update(new_unnamed)
    save_notified(notified)


# â”€â”€â”€ ã‚·ãƒ¼ãƒˆåŒæœŸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def sync_to_sheet(dry_run=False):
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã‚·ãƒ¼ãƒˆã‚’åŒæœŸ"""
    # 1. ãƒ•ã‚©ãƒ«ãƒ€ã‚¹ã‚­ãƒ£ãƒ³
    csv_files, unnamed_files = scan_csv_folder()

    # æ—¥ä»˜ä¸æ˜ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒã‚§ãƒƒã‚¯
    check_unnamed_files(unnamed_files, dry_run=dry_run)

    if not csv_files:
        logger.info("æ—¥ä»˜ä»˜ãCSVãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
        return 0

    logger.info(f"æ—¥ä»˜ä»˜ãCSV: {len(csv_files)} ãƒ•ã‚¡ã‚¤ãƒ«")

    # 2. ã‚·ãƒ¼ãƒˆèª­ã¿è¾¼ã¿
    spreadsheet_id, gid = extract_spreadsheet_id(SPREADSHEET_URL)
    client = get_client(ACCOUNT)
    spreadsheet = client.open_by_key(spreadsheet_id)
    ws = next((w for w in spreadsheet.worksheets() if w.id == gid), None)
    if ws is None:
        ws = spreadsheet.worksheet(SHEET_NAME)

    data = ws.get_all_values()
    if not data:
        logger.error("ã‚·ãƒ¼ãƒˆãŒç©ºã§ã™")
        return 0

    # 2.5. ã‚·ãƒ¼ãƒˆã«ãªã„æ—¥ä»˜ã®CSVãŒã‚ã‚Œã°è¡Œã‚’è‡ªå‹•è¿½åŠ 
    existing_dates = {row[0] for row in data[1:]}  # ãƒ˜ãƒƒãƒ€ãƒ¼é™¤ã
    new_dates = sorted(d for d in csv_files if d not in existing_dates)

    if new_dates and not dry_run:
        new_rows = [
            [d, f"{BASE_CSV_NAME}.csv", "", "è¦ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"]
            for d in new_dates
        ]
        ws.append_rows(new_rows, value_input_option="USER_ENTERED")
        logger.info(f"å…ƒãƒ‡ãƒ¼ã‚¿ã«æ–°è¦è¡Œè¿½åŠ : {len(new_dates)} ä»¶ ({new_dates[0]} ã€œ {new_dates[-1]})")
        # è¿½åŠ å¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚’å†èª­ã¿è¾¼ã¿
        data = ws.get_all_values()
    elif new_dates:
        logger.info(f"(dry-run) å…ƒãƒ‡ãƒ¼ã‚¿ã«æ–°è¦è¡Œè¿½åŠ äºˆå®š: {len(new_dates)} ä»¶ ({new_dates[0]} ã€œ {new_dates[-1]})")

    # 3. æ›´æ–°å¯¾è±¡ã‚’ç‰¹å®š
    now = datetime.now().strftime("%Y-%m-%d %H:%M")
    updates = []
    update_rows = []

    for i, row in enumerate(data[1:], start=2):  # skip header
        target_date = row[0]
        status = row[3] if len(row) > 3 else ""

        if status == "å®Œäº†":
            continue  # æ—¢ã«å®Œäº†æ¸ˆã¿ã¯ã‚¹ã‚­ãƒƒãƒ—

        if target_date in csv_files:
            filename = csv_files[target_date]
            updates.append({
                "row": i,
                "date": target_date,
                "filename": filename,
            })
            update_rows.append([filename, now, "å®Œäº†"])

    if not updates:
        logger.info("æ›´æ–°å¯¾è±¡ãªã—ï¼ˆã™ã¹ã¦å®Œäº†æ¸ˆã¿ or CSVãªã—ï¼‰")
        return 0

    logger.info(f"æ›´æ–°å¯¾è±¡: {len(updates)} è¡Œ")
    for u in updates[:5]:
        logger.info(f"  {u['date']} â†’ {u['filename']}")
    if len(updates) > 5:
        logger.info(f"  ... ä»– {len(updates) - 5} è¡Œ")

    if dry_run:
        logger.info("(dry-run: æ›¸ãè¾¼ã¿ã‚¹ã‚­ãƒƒãƒ—)")
        return len(updates)

    # 4. ä¸€æ‹¬æ›¸ãè¾¼ã¿
    first_row = updates[0]["row"]
    last_row = updates[-1]["row"]

    if last_row - first_row + 1 == len(updates):
        # é€£ç¶šè¡Œ â†’ ä¸€æ‹¬æ›´æ–°
        range_notation = f"B{first_row}:D{last_row}"
        ws.update(range_notation, update_rows)
        logger.info(f"ä¸€æ‹¬æ›¸ãè¾¼ã¿å®Œäº†: {range_notation}")
    else:
        # é£›ã³é£›ã³ â†’ å…¨è¡Œåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½œã£ã¦ä¸€æ‹¬æ›´æ–°
        all_rows = []
        update_map = {u["row"]: idx for idx, u in enumerate(updates)}
        for i, row in enumerate(data[1:], start=2):
            if i in update_map:
                all_rows.append(update_rows[update_map[i]])
            else:
                all_rows.append([row[1] if len(row) > 1 else "",
                                 row[2] if len(row) > 2 else "",
                                 row[3] if len(row) > 3 else ""])
        range_notation = f"B2:D{len(data)}"
        ws.update(range_notation, all_rows)
        logger.info(f"ä¸€æ‹¬æ›¸ãè¾¼ã¿å®Œäº†: {range_notation}")

    # 5. æ›´æ–°å®Œäº†ã‚’LINEé€šçŸ¥
    dates = [u["date"] for u in updates]
    first = dates[0]
    last = dates[-1]
    message = f"ğŸ“Š å…ƒãƒ‡ãƒ¼ã‚¿ã‚·ãƒ¼ãƒˆæ›´æ–°å®Œäº†: {len(updates)} ä»¶\n{first} ã€œ {last}"
    send_line_notify(message)

    return len(updates)


# â”€â”€â”€ ã‚¹ã‚­ãƒ«ãƒ—ãƒ©ã‚¹ï¼ˆæ—¥åˆ¥ï¼‰æ§‹ç¯‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def parse_number(val):
    """CSV ã®å€¤ã‚’æ•°å€¤ã«å¤‰æ›ã€‚ç©ºæ–‡å­—ãƒ»å¤‰æ›ä¸å¯ã¯ 0"""
    if not val or val.strip() == "":
        return 0
    try:
        n = float(val)
        return int(n) if n == int(n) else round(n, 2)
    except (ValueError, OverflowError):
        return 0


def read_all_csvs():
    """å…¨CSVã‚’èª­ã¿è¾¼ã¿ã€æ—¥ä»˜ä»˜ãã®è¡Œãƒªã‚¹ãƒˆã‚’è¿”ã™"""
    if not os.path.isdir(CSV_DIR):
        logger.error(f"ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {CSV_DIR}")
        return []

    all_rows = []
    files = sorted(f for f in os.listdir(CSV_DIR)
                   if DATE_PATTERN.match(f) and BASE_CSV_NAME in f)

    for fname in files:
        date_str = DATE_PATTERN.match(fname).group(1)
        path = os.path.join(CSV_DIR, fname)
        with open(path, encoding="utf-8-sig") as fh:
            reader = csv.reader(fh)
            header = next(reader, None)
            if not header:
                continue
            for row in reader:
                if len(row) < 12:
                    continue
                # [æ—¥ä»˜, å¤§ã‚«ãƒ†ã‚´ãƒª, é›†å®¢åª’ä½“, ãƒ•ã‚¡ãƒãƒ«å,
                #  é›†å®¢æ•°, å€‹åˆ¥äºˆç´„æ•°, å®Ÿæ–½æ•°, å£²ä¸Š, åºƒå‘Šè²»,
                #  CPA, å€‹åˆ¥CPO, å˜æœˆROAS, å˜æœˆLTV]
                all_rows.append([
                    date_str,
                    row[0],                # å¤§ã‚«ãƒ†ã‚´ãƒª
                    row[1],                # é›†å®¢åª’ä½“
                    row[2],                # ãƒ•ã‚¡ãƒãƒ«å
                    parse_number(row[3]),   # é›†å®¢æ•°
                    parse_number(row[4]),   # å€‹åˆ¥äºˆç´„æ•°
                    parse_number(row[5]),   # å®Ÿæ–½æ•°
                    parse_number(row[6]),   # å£²ä¸Š
                    parse_number(row[7]),   # åºƒå‘Šè²»
                    parse_number(row[8]),   # CPA
                    parse_number(row[9]),   # å€‹åˆ¥CPO
                    parse_number(row[10]),  # å˜æœˆROAS
                    parse_number(row[11]),  # å˜æœˆLTV
                ])

    logger.info(f"CSVèª­ã¿è¾¼ã¿: {len(files)} ãƒ•ã‚¡ã‚¤ãƒ«, {len(all_rows)} è¡Œ")
    return all_rows


def build_daily_sheet(dry_run=False):
    """å…¨CSVãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€ã‚¹ã‚­ãƒ«ãƒ—ãƒ©ã‚¹ï¼ˆæ—¥åˆ¥ï¼‰ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã‚€"""
    # 1. å…¨CSVèª­ã¿è¾¼ã¿
    all_rows = read_all_csvs()
    if not all_rows:
        logger.error("CSVãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return 0

    # æ—¥ä»˜ã®ç¯„å›²
    dates = sorted(set(r[0] for r in all_rows))
    logger.info(f"æœŸé–“: {dates[0]} ã€œ {dates[-1]} ({len(dates)} æ—¥)")
    logger.info(f"åˆè¨ˆ: {len(all_rows)} è¡Œ")

    if dry_run:
        logger.info("(dry-run: æ›¸ãè¾¼ã¿ã‚¹ã‚­ãƒƒãƒ—)")
        return len(all_rows)

    # 2. ã‚·ãƒ¼ãƒˆã«æ¥ç¶š
    client = get_client(ACCOUNT)
    spreadsheet = client.open_by_key(SPREADSHEET_ID)
    ws = spreadsheet.worksheet(DAILY_SHEET_NAME)

    # 3. ã‚·ãƒ¼ãƒˆã‚’ãƒªã‚µã‚¤ã‚ºï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼4è¡Œ + ãƒ‡ãƒ¼ã‚¿è¡Œ + ä½™è£•100è¡Œï¼‰
    needed_rows = 4 + len(all_rows) + 100
    current_rows = ws.row_count
    if needed_rows > current_rows:
        ws.resize(rows=needed_rows)
        logger.info(f"ã‚·ãƒ¼ãƒˆãƒªã‚µã‚¤ã‚º: {current_rows} â†’ {needed_rows} è¡Œ")

    # 4. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢ï¼ˆè¡Œ5ä»¥é™ï¼‰
    ws.batch_clear([f"A5:M{current_rows}"])
    logger.info("æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªã‚¢å®Œäº†")

    # 5. ãƒ‡ãƒ¼ã‚¿æ›¸ãè¾¼ã¿ï¼ˆè¡Œ5ã€œï¼‰
    # Google Sheets API ã¯ 1ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚ãŸã‚Šä¸Šé™ã‚ã‚‹ãŸã‚ã€1000è¡Œãšã¤åˆ†å‰²
    BATCH_SIZE = 1000
    for i in range(0, len(all_rows), BATCH_SIZE):
        batch = all_rows[i:i + BATCH_SIZE]
        start_row = 5 + i
        end_row = start_row + len(batch) - 1
        range_notation = f"A{start_row}:M{end_row}"
        ws.update(range_notation, batch, value_input_option="USER_ENTERED")
        logger.info(f"æ›¸ãè¾¼ã¿: {range_notation} ({len(batch)} è¡Œ)")

    # 6. æœ€çµ‚æ›´æ–°æ—¥ã‚’æ›´æ–°ï¼ˆè¡Œ2ï¼‰
    now = datetime.now().strftime("%Y-%m-%d %H:%M")
    ws.update_acell("A2", f"æœ€çµ‚æ›´æ–°: {now}")

    logger.info(f"ã‚¹ã‚­ãƒ«ãƒ—ãƒ©ã‚¹ï¼ˆæ—¥åˆ¥ï¼‰æ§‹ç¯‰å®Œäº†: {len(all_rows)} è¡Œ")

    # 7. LINEé€šçŸ¥
    message = (
        f"ğŸ“Š ã‚¹ã‚­ãƒ«ãƒ—ãƒ©ã‚¹ï¼ˆæ—¥åˆ¥ï¼‰ã‚·ãƒ¼ãƒˆæ›´æ–°å®Œäº†\n"
        f"{dates[0]} ã€œ {dates[-1]}\n"
        f"{len(dates)} æ—¥åˆ† / {len(all_rows)} è¡Œ"
    )
    send_line_notify(message)

    return len(all_rows)


# â”€â”€â”€ ã‚¹ã‚­ãƒ«ãƒ—ãƒ©ã‚¹ï¼ˆæœˆåˆ¥ï¼‰æ§‹ç¯‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def build_monthly_sheet(dry_run=False):
    """æ—¥åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚’æœˆå˜ä½ã§é›†è¨ˆã—ã€ã‚¹ã‚­ãƒ«ãƒ—ãƒ©ã‚¹ï¼ˆæœˆåˆ¥ï¼‰ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã‚€"""
    from collections import defaultdict

    # 1. å…¨CSVèª­ã¿è¾¼ã¿
    all_rows = read_all_csvs()
    if not all_rows:
        logger.error("CSVãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return 0

    # 2. æœˆã”ã¨ã«é›†è¨ˆï¼ˆé›†å®¢æ•°, äºˆç´„æ•°, å®Ÿæ–½æ•°, å£²ä¸Š, åºƒå‘Šè²»ï¼‰
    monthly = defaultdict(lambda: {
        "é›†å®¢æ•°": 0, "äºˆç´„æ•°": 0, "å®Ÿæ–½æ•°": 0, "å£²ä¸Š": 0, "åºƒå‘Šè²»": 0
    })

    for row in all_rows:
        # row: [æ—¥ä»˜, å¤§ã‚«ãƒ†ã‚´ãƒª, é›†å®¢åª’ä½“, ãƒ•ã‚¡ãƒãƒ«å, é›†å®¢æ•°, äºˆç´„æ•°, å®Ÿæ–½æ•°, å£²ä¸Š, åºƒå‘Šè²», CPA, CPO, ROAS, LTV]
        month_key = row[0][:7]  # "2025-07-01" â†’ "2025-07"
        monthly[month_key]["é›†å®¢æ•°"] += row[4]
        monthly[month_key]["äºˆç´„æ•°"] += row[5]
        monthly[month_key]["å®Ÿæ–½æ•°"] += row[6]
        monthly[month_key]["å£²ä¸Š"] += row[7]
        monthly[month_key]["åºƒå‘Šè²»"] += row[8]

    # 3. KPIè¨ˆç®— & è¡Œãƒ‡ãƒ¼ã‚¿ä½œæˆ
    sheet_rows = []
    for month in sorted(monthly.keys()):
        m = monthly[month]
        é›†å®¢æ•° = m["é›†å®¢æ•°"]
        äºˆç´„æ•° = m["äºˆç´„æ•°"]
        å®Ÿæ–½æ•° = m["å®Ÿæ–½æ•°"]
        å£²ä¸Š = m["å£²ä¸Š"]
        åºƒå‘Šè²» = m["åºƒå‘Šè²»"]

        cpa = round(åºƒå‘Šè²» / é›†å®¢æ•°) if é›†å®¢æ•° > 0 else 0
        cpo = round(åºƒå‘Šè²» / äºˆç´„æ•°) if äºˆç´„æ•° > 0 else 0
        roas = round(å£²ä¸Š / åºƒå‘Šè²» * 100, 1) if åºƒå‘Šè²» > 0 else 0
        ltv = round(å£²ä¸Š / é›†å®¢æ•°) if é›†å®¢æ•° > 0 else 0
        ç²—åˆ© = å£²ä¸Š - åºƒå‘Šè²»

        sheet_rows.append([
            month, é›†å®¢æ•°, äºˆç´„æ•°, å®Ÿæ–½æ•°, å£²ä¸Š, åºƒå‘Šè²»,
            cpa, cpo, roas, ltv, ç²—åˆ©
        ])

    logger.info(f"æœˆåˆ¥é›†è¨ˆ: {len(sheet_rows)} ãƒ¶æœˆ")
    for r in sheet_rows:
        logger.info(f"  {r[0]}: é›†å®¢{r[1]:,} å£²ä¸ŠÂ¥{r[4]:,} åºƒå‘Šè²»Â¥{r[5]:,} ROAS{r[8]}%")

    if dry_run:
        logger.info("(dry-run: æ›¸ãè¾¼ã¿ã‚¹ã‚­ãƒƒãƒ—)")
        return len(sheet_rows)

    # 4. ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã¿
    client = get_client(ACCOUNT)
    spreadsheet = client.open_by_key(SPREADSHEET_ID)
    ws = spreadsheet.worksheet(MONTHLY_SHEET_NAME)

    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªã‚¢ï¼ˆè¡Œ5ä»¥é™ï¼‰
    current_rows = ws.row_count
    ws.batch_clear([f"A5:K{current_rows}"])

    # ãƒ‡ãƒ¼ã‚¿æ›¸ãè¾¼ã¿
    last_row = 4 + len(sheet_rows)
    ws.update(f"A5:K{last_row}", sheet_rows, value_input_option="USER_ENTERED")

    # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆé©ç”¨
    formats = [
        (f"B5:D{last_row}", {"type": "NUMBER", "pattern": "#,##0"}),
        (f"E5:F{last_row}", {"type": "CURRENCY", "pattern": "Â¥#,##0"}),
        (f"G5:H{last_row}", {"type": "CURRENCY", "pattern": "Â¥#,##0"}),
        (f"I5:I{last_row}", {"type": "NUMBER", "pattern": "0.0\"%\""}),
        (f"J5:J{last_row}", {"type": "CURRENCY", "pattern": "Â¥#,##0"}),
        (f"K5:K{last_row}", {"type": "CURRENCY", "pattern": "Â¥#,##0"}),
    ]
    for cell_range, num_fmt in formats:
        ws.format(cell_range, {"numberFormat": num_fmt})

    # æœ€çµ‚æ›´æ–°æ—¥
    now = datetime.now().strftime("%Y-%m-%d %H:%M")
    ws.update_acell("A2", f"æœ€çµ‚æ›´æ–°: {now}")

    logger.info(f"ã‚¹ã‚­ãƒ«ãƒ—ãƒ©ã‚¹ï¼ˆæœˆåˆ¥ï¼‰æ§‹ç¯‰å®Œäº†: {len(sheet_rows)} ãƒ¶æœˆ")

    # LINEé€šçŸ¥
    months = [r[0] for r in sheet_rows]
    message = (
        f"ğŸ“Š ã‚¹ã‚­ãƒ«ãƒ—ãƒ©ã‚¹ï¼ˆæœˆåˆ¥ï¼‰ã‚·ãƒ¼ãƒˆæ›´æ–°å®Œäº†\n"
        f"{months[0]} ã€œ {months[-1]} ({len(months)} ãƒ¶æœˆ)"
    )
    send_line_notify(message)

    return len(sheet_rows)


# â”€â”€â”€ KPIã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”Ÿæˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def generate_kpi_cache(dry_run=False):
    """å…¨CSVãƒ‡ãƒ¼ã‚¿ã‹ã‚‰KPIã‚µãƒãƒªãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆJSONï¼‰ã‚’ç”Ÿæˆã™ã‚‹ã€‚
    AIç§˜æ›¸ãŒã‚·ãƒ¼ãƒˆå‚ç…§ãªã—ã§å³åº§ã«KPIã‚’å›ç­”ã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ã€‚"""
    from collections import defaultdict

    all_rows = read_all_csvs()
    if not all_rows:
        logger.error("CSVãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ â†’ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”Ÿæˆã‚¹ã‚­ãƒƒãƒ—")
        return False

    # â”€â”€ 1. æœˆåˆ¥ã‚µãƒãƒªï¼ˆå…¨ä½“ï¼‰ â”€â”€
    monthly = defaultdict(lambda: {"é›†å®¢æ•°": 0, "äºˆç´„æ•°": 0, "å®Ÿæ–½æ•°": 0, "å£²ä¸Š": 0, "åºƒå‘Šè²»": 0})
    for row in all_rows:
        mk = row[0][:7]
        monthly[mk]["é›†å®¢æ•°"] += row[4]
        monthly[mk]["äºˆç´„æ•°"] += row[5]
        monthly[mk]["å®Ÿæ–½æ•°"] += row[6]
        monthly[mk]["å£²ä¸Š"] += row[7]
        monthly[mk]["åºƒå‘Šè²»"] += row[8]

    monthly_list = []
    for month in sorted(monthly.keys()):
        m = monthly[month]
        é›†å®¢ = m["é›†å®¢æ•°"]; äºˆç´„ = m["äºˆç´„æ•°"]; å®Ÿæ–½ = m["å®Ÿæ–½æ•°"]
        å£²ä¸Š = m["å£²ä¸Š"]; åºƒå‘Šè²» = m["åºƒå‘Šè²»"]
        monthly_list.append({
            "month": month,
            "é›†å®¢æ•°": é›†å®¢, "å€‹åˆ¥äºˆç´„æ•°": äºˆç´„, "å®Ÿæ–½æ•°": å®Ÿæ–½,
            "å£²ä¸Š": å£²ä¸Š, "åºƒå‘Šè²»": åºƒå‘Šè²»,
            "CPA": round(åºƒå‘Šè²» / é›†å®¢) if é›†å®¢ > 0 else 0,
            "CPO": round(åºƒå‘Šè²» / äºˆç´„) if äºˆç´„ > 0 else 0,
            "ROAS": round(å£²ä¸Š / åºƒå‘Šè²» * 100, 1) if åºƒå‘Šè²» > 0 else 0,
            "LTV": round(å£²ä¸Š / é›†å®¢) if é›†å®¢ > 0 else 0,
            "ç²—åˆ©": å£²ä¸Š - åºƒå‘Šè²»,
        })

    # â”€â”€ 2. æœˆåˆ¥Ã—åª’ä½“ å†…è¨³ â”€â”€
    media_monthly = defaultdict(lambda: defaultdict(lambda: {"é›†å®¢æ•°": 0, "äºˆç´„æ•°": 0, "å£²ä¸Š": 0, "åºƒå‘Šè²»": 0}))
    for row in all_rows:
        mk = row[0][:7]
        media = row[2]  # é›†å®¢åª’ä½“
        if not media:
            continue
        media_monthly[mk][media]["é›†å®¢æ•°"] += row[4]
        media_monthly[mk][media]["äºˆç´„æ•°"] += row[5]
        media_monthly[mk][media]["å£²ä¸Š"] += row[7]
        media_monthly[mk][media]["åºƒå‘Šè²»"] += row[8]

    monthly_by_media = {}
    for mk in sorted(media_monthly.keys()):
        monthly_by_media[mk] = {}
        for media, vals in sorted(media_monthly[mk].items()):
            monthly_by_media[mk][media] = {
                "é›†å®¢æ•°": vals["é›†å®¢æ•°"], "äºˆç´„æ•°": vals["äºˆç´„æ•°"],
                "å£²ä¸Š": vals["å£²ä¸Š"], "åºƒå‘Šè²»": vals["åºƒå‘Šè²»"],
                "ROAS": round(vals["å£²ä¸Š"] / vals["åºƒå‘Šè²»"] * 100, 1) if vals["åºƒå‘Šè²»"] > 0 else 0,
            }

    # â”€â”€ 3. ç›´è¿‘14æ—¥ æ—¥åˆ¥åˆè¨ˆ â”€â”€
    daily_totals = defaultdict(lambda: {"é›†å®¢æ•°": 0, "äºˆç´„æ•°": 0, "å£²ä¸Š": 0, "åºƒå‘Šè²»": 0})
    for row in all_rows:
        dt = row[0]
        daily_totals[dt]["é›†å®¢æ•°"] += row[4]
        daily_totals[dt]["äºˆç´„æ•°"] += row[5]
        daily_totals[dt]["å£²ä¸Š"] += row[7]
        daily_totals[dt]["åºƒå‘Šè²»"] += row[8]

    sorted_dates = sorted(daily_totals.keys(), reverse=True)[:14]
    recent_daily = []
    for dt in sorted_dates:
        d = daily_totals[dt]
        recent_daily.append({
            "date": dt,
            "é›†å®¢æ•°": d["é›†å®¢æ•°"], "å€‹åˆ¥äºˆç´„æ•°": d["äºˆç´„æ•°"],
            "å£²ä¸Š": d["å£²ä¸Š"], "åºƒå‘Šè²»": d["åºƒå‘Šè²»"],
            "ROAS": round(d["å£²ä¸Š"] / d["åºƒå‘Šè²»"] * 100, 1) if d["åºƒå‘Šè²»"] > 0 else 0,
        })

    # â”€â”€ 4. ç›´è¿‘14æ—¥ æ—¥åˆ¥Ã—åª’ä½“ â”€â”€
    media_daily = defaultdict(lambda: defaultdict(lambda: {"é›†å®¢æ•°": 0, "å£²ä¸Š": 0, "åºƒå‘Šè²»": 0}))
    for row in all_rows:
        dt = row[0]
        if dt not in sorted_dates:
            continue
        media = row[2]
        if not media:
            continue
        media_daily[dt][media]["é›†å®¢æ•°"] += row[4]
        media_daily[dt][media]["å£²ä¸Š"] += row[7]
        media_daily[dt][media]["åºƒå‘Šè²»"] += row[8]

    recent_daily_by_media = {}
    for dt in sorted_dates:
        recent_daily_by_media[dt] = {}
        for media, vals in sorted(media_daily[dt].items()):
            recent_daily_by_media[dt][media] = {
                "é›†å®¢æ•°": vals["é›†å®¢æ•°"], "å£²ä¸Š": vals["å£²ä¸Š"], "åºƒå‘Šè²»": vals["åºƒå‘Šè²»"],
            }

    # â”€â”€ 5. JSONå‡ºåŠ› â”€â”€
    cache = {
        "updated_at": datetime.now().isoformat(timespec="seconds"),
        "monthly": monthly_list,
        "monthly_by_media": monthly_by_media,
        "recent_daily": recent_daily,
        "recent_daily_by_media": recent_daily_by_media,
    }

    if dry_run:
        logger.info(f"(dry-run) KPIã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”Ÿæˆäºˆå®š: {len(monthly_list)}ãƒ¶æœˆ, {len(recent_daily)}æ—¥åˆ†")
        return True

    with open(KPI_CACHE_PATH, "w", encoding="utf-8") as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)

    logger.info(f"KPIã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”Ÿæˆå®Œäº†: {KPI_CACHE_PATH} ({len(monthly_list)}ãƒ¶æœˆ, {len(recent_daily)}æ—¥åˆ†)")
    return True


# â”€â”€â”€ CLI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if __name__ == "__main__":
    args = sys.argv[1:]
    dry_run = "--dry-run" in args

    try:
        if "cache" in args:
            # KPIã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã¿å†ç”Ÿæˆ
            generate_kpi_cache(dry_run=dry_run)
        elif "build" in args:
            # ã‚¹ã‚­ãƒ«ãƒ—ãƒ©ã‚¹ï¼ˆæ—¥åˆ¥ï¼‰ã‚·ãƒ¼ãƒˆã®ã¿æ§‹ç¯‰
            count = build_daily_sheet(dry_run=dry_run)
            if count > 0:
                logger.info(f"å®Œäº†: {count} è¡Œæ›¸ãè¾¼ã¿")
                generate_kpi_cache(dry_run=dry_run)
        elif "monthly" in args:
            # ã‚¹ã‚­ãƒ«ãƒ—ãƒ©ã‚¹ï¼ˆæœˆåˆ¥ï¼‰ã‚·ãƒ¼ãƒˆã®ã¿æ§‹ç¯‰
            count = build_monthly_sheet(dry_run=dry_run)
            if count > 0:
                logger.info(f"å®Œäº†: {count} ãƒ¶æœˆåˆ†æ›¸ãè¾¼ã¿")
                generate_kpi_cache(dry_run=dry_run)
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: å…ƒãƒ‡ãƒ¼ã‚¿ â†’ æ—¥åˆ¥ â†’ æœˆåˆ¥ ã®é€£é–å®Ÿè¡Œ
            count = sync_to_sheet(dry_run=dry_run)
            if count > 0:
                logger.info(f"å…ƒãƒ‡ãƒ¼ã‚¿: {count} è¡Œæ›´æ–° â†’ æ—¥åˆ¥ãƒ»æœˆåˆ¥ã‚’å†æ§‹ç¯‰")
                daily = build_daily_sheet(dry_run=dry_run)
                logger.info(f"æ—¥åˆ¥: {daily} è¡Œæ›¸ãè¾¼ã¿ â†’ æœˆåˆ¥ã‚’å†é›†è¨ˆ")
                monthly = build_monthly_sheet(dry_run=dry_run)
                logger.info(f"æœˆåˆ¥: {monthly} ãƒ¶æœˆåˆ†æ›¸ãè¾¼ã¿ â†’ KPIã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”Ÿæˆ")
                generate_kpi_cache(dry_run=dry_run)
            else:
                logger.info("å…ƒãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›´ãªã— â†’ æ—¥åˆ¥ãƒ»æœˆåˆ¥ã®æ›´æ–°ã‚¹ã‚­ãƒƒãƒ—")
        sys.exit(0)
    except Exception as e:
        logger.error(f"ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        sys.exit(1)
